{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba979be6-e1d8-48de-9492-75a3645d607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import DeepLake\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "from langchain.llms import GPT4All\n",
    "# from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from tqdm import tqdm\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import validator\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import cohere\n",
    "from langchain.embeddings import CohereEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73db0601-8230-4b5a-bd15-685e72b92764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1224cec9-281e-4c88-99f1-95089b65ddbd",
   "metadata": {},
   "source": [
    "1. How to add more documents to chroma db?\n",
    "2. What steps are taken when an agent is run?\n",
    "3. What are the parameters that is responsible for the output any LLM generates? e.g: temperature, probability, etc.\n",
    "4. Zero-shot vs few-shot prompts\n",
    "5. Ways to ask multiple questions in one go?\n",
    "6. How to use other open-source models apart from openai models?\n",
    "7. Difference between .run(), .generate(), and .predict()?\n",
    "8. Go through variuos chains in the presentation and how we can make these better with right prompts?\n",
    "9. What is the difference between LLM and Chat models?\n",
    "10. How to ask new questions to Chat model?\n",
    "11. sampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000 generate: n_ctx = 512, n_batch = 1, n_predict = 256, n_keep = 0: \\\n",
    "    How to change arguments from above list for GPT4ALL model?\n",
    "12. Various example selectors for Few shot prompts.\n",
    "13. Making best use of GPT4ALL models.\n",
    "14. Why large open-source models timw_out with HuggingFaceHub but working fine with HuggingFacePipeline?\n",
    "15. What are different chain_type such as stuff,map_reduce, refine, etc.?\n",
    "16. What is the use of LLMChainExtractor and ContextualCompressionRetriever?\n",
    "17. Read about various data loaders:\n",
    "    * including TextLoader, PyPDFLoader, SeleniumURLLoader, and Google Drive Loader.\n",
    "18. Read about various Text splitters:\n",
    "    * CharacterTextSplitter, RecursiveCharacterTextSplitter, NLTKTextSplitter, SpacyTextSplitter, MarkdownTextSplitter, TokenTextSplitter, etc.\n",
    "19. Read about various embedding models/platforms:\n",
    "    * OpenAI, HuggingFace, Cohere, etc.\n",
    "20. Building a customer QA chatbot using walmart data\n",
    "21. Avoid adding duplicate docs in chromadb index.\n",
    "22. Read about various chains:\n",
    "    * LLMChain, ConversationChain, SequentialChain, TransformationChain, LLMCheckerChain, LLMSummarizationChain, Custom Chain, etc.\n",
    "23. Providing custom prompt to load_summarize_chain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a4ae5ef-211e-478d-9e38-1ade83f87446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# load_dotenv(find_dotenv())\n",
    "# os.getenv('OPENAI_API_KEY')\n",
    "# os.getenv('ACTIVELOOP_TOKEN')\n",
    "# os.getenv('GOOGLE_API_KEY')\n",
    "# os.getenv('GOOGLE_CSE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85c9e05e-49a9-4d56-a842-5347557658e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"\"\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"]= \"\"\n",
    "os.environ[\"GOOGLE_CSE_ID\"]= \"\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]= \"\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"\"\n",
    "os.environ[\"WOLFRAM_ALPHA_APPID\"] = \"\"\n",
    "os.environ[\"SERPAPI_API_KEY\"]= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24e714d3-9a74-4ea3-8dc0-3b6ef3fd2a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "os.environ['http_proxy'] = \"http://sysproxy.wal-mart.com:8080\"\n",
    "os.environ['https_proxy'] = \"http://sysproxy.wal-mart.com:8080\"\n",
    "# os.environ['SSL_CERT_FILE'] = '/etc/ssl/certs/ca-bundle.crt'\n",
    "# os.environ['NO_PROXY'] = \"*.walmart.com,*.wal-mart.com,*.walmart.net,*.wal-mart.net,localhost,127.0.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71af81e5-41b8-48e5-8ffb-3fcbbb8420b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/libraries/llm_exp_1/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3480d18-2ea9-40e1-b391-e1d4aefa8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Suggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities.\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d66f88c-3cfd-484d-b014-4e5f3284226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"eco-friendly water bottles\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d41319-a757-4b43-9712-6270c0cafd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "# Start the conversation\n",
    "conversation.predict(input=\"Tell me about yourself.\")\n",
    "\n",
    "# Continue the conversation\n",
    "conversation.predict(input=\"What can you do?\")\n",
    "conversation.predict(input=\"How can you help me with data analysis?\")\n",
    "\n",
    "# Display the conversation\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97439d24-4850-457d-8193-9888819fd43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the LLM and embeddings models\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "# embeddings = embedding_functions.OpenAIEmbeddingFunction(model_name=\"text-embedding-ada-002\", api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "# embeddings = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create our documents\n",
    "texts = [\n",
    "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
    "    \"Louis XIV was born in 5 September 1638\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)\n",
    "\n",
    "# load it into Chroma\n",
    "# db = Chroma.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2d1ae-b88d-4c92-8c2d-f8e2bd22393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.get(include=['embeddings', 'documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fedcd0a-274a-4d7a-b1e7-4478d555d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chromadb\n",
    "# persistent_client = chromadb.Client()\n",
    "# collection = persistent_client.get_or_create_collection(\"collection_name\")\n",
    "# collection.add(ids=[\"1\", \"2\"], documents= docs)\n",
    "\n",
    "# langchain_chroma = Chroma(\n",
    "#     client=persistent_client,\n",
    "#     collection_name=\"collection_name\",\n",
    "#     embedding_function=embeddings,\n",
    "# )\n",
    "\n",
    "# print(\"There are\", langchain_chroma._collection.count(), \"in the collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226edf9-9c8a-4a68-a991-01ad7036c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_client = chromadb.Client()\n",
    "db_openai = Chroma.from_documents(\n",
    "    docs, embeddings, client=tmp_client, collection_name=\"openai_collection\"\n",
    ")\n",
    "\n",
    "print(\"There are\", db_openai._collection.count(), \"in the collection\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4745c0-e3b2-4c5c-b094-bb0410dbec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_openai.get(include=['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42fd5d-8113-4ea2-bc72-f12b20b2b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_client.get_collection('openai_collection')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd556a8-7aa7-48bb-8b79-99ddc907590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=db_openai.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad9f00-c409-4985-98ce-4eab891710da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "    name=\"Retrieval QA System\",\n",
    "        func=retrieval_qa.run,\n",
    "        description=\"Useful for answering questions.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0bfbc-fd3a-47ea-87fe-179964a6b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.run(\"When was Napoleone born?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4295cace-710c-44a4-b5d5-fcc566f38f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new documents\n",
    "texts = [\n",
    "    \"Lady Gaga was born in 28 March 1986\",\n",
    "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5816195e-e726-41da-bca0-e56ba63f5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai_lc_client.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d75cb0-4ec4-4045-95e3-83fad3e22b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai_lc_client.add_documents(docs, embeddings, client=new_client, collection_name=\"openai_collection\")\n",
    "db_openai= Chroma.from_documents(\n",
    "    docs, embeddings, client=tmp_client, collection_name=\"openai_collection\"\n",
    ")\n",
    "print(\"There are\", db_openai._collection.count(), \"in the collection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a7c1a7-6196-4c4b-81c6-648c3811ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.run(\"When was Michael Jordan born?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a0dc9-6ebe-4d10-957f-2883190d4f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_openai.get(include=['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f891af-7d8c-41be-bf21-f82cbcb41742",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.run(\"Who is Lady Gaga\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b2d3b1-deb3-46ac-af2a-163d7fedc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to set the environment variables\n",
    "# “GOOGLE_API_KEY” and “GOOGLE_CSE_ID” to be able to use\n",
    "# Google Search via API.\n",
    "search = GoogleSearchAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef446b2-afd4-43f8-a390-5d8d36ed6f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name = \"google-search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to search google to answer questions about current events\"\n",
    "    )\n",
    "]\n",
    "\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True,\n",
    "                         max_iterations=6)\n",
    "\n",
    "\n",
    "response = agent(\"What's the latest news about the Mars rover?\")\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df018aa-87e0-48a7-872d-1c20faa0b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"Write a summary of the following text: {query}\"\n",
    ")\n",
    "\n",
    "summarize_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# remember to set the environment variables\n",
    "# “GOOGLE_API_KEY” and “GOOGLE_CSE_ID” to be able to use\n",
    "# Google Search via API.\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for finding information about recent events\"\n",
    "    ),\n",
    "    Tool(\n",
    "       name='Summarizer',\n",
    "       func=summarize_chain.run,\n",
    "       description='useful for summarizing texts'\n",
    "    )\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True  \n",
    ")\n",
    "\n",
    "response = agent(\"What's the latest news about the Mars rover? Then please summarize the results.\")\n",
    "print(response['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7088154a-7e02-4be1-9582-f95df66a0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", n=2, best_of=2)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39598f-7f92-4845-b4f5-cb1006793602",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the weather like?\",\n",
    "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
    "    }, {\n",
    "        \"query\": \"How old are you?\",\n",
    "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create an example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for its humor and wit, providing\n",
    "entertaining and amusing responses to users' questions. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few-shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "# load the model\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
    "chain.run(\"What's the capital of Democratic Republic of Congo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365935a-a301-45ca-ab78-ffaffa0f33f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"What is the capital city of France?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d575c-1871-41bf-add8-ea8f9963e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "        repo_id='google/flan-t5-large',\n",
    "    model_kwargs={'temperature':0}\n",
    ")\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "# ask the user question about the capital of France\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416921fb-c751-46f3-8cf6-4d9dd9ffb599",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = [\n",
    "    {'question': \"What is the capital city of France?\"},\n",
    "    {'question': \"What is the largest mammal on Earth?\"},\n",
    "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question': \"What color is a ripe banana?\"}\n",
    "]\n",
    "res = llm_chain.generate(qa)\n",
    "print( res )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da8865-91a1-471b-b906-7c6b1dc563c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"What is the capital city of France?\\n\" +\n",
    "    \"What is the largest mammal on Earth?\\n\" +\n",
    "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
    "\t\"What color is a ripe banana?\\n\"\n",
    ")\n",
    "llm_chain.run(qs_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082f387f-b074-41f9-bf80-f3811c91981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "summarization_template = \"Summarize the following text to one sentence at maximum: {text}\"\n",
    "summarization_prompt = PromptTemplate(input_variables=[\"text\"], template=summarization_template)\n",
    "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)\n",
    "text = \"LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\"\n",
    "summarized_text = summarization_chain.predict(text=text)\n",
    "# summarized_text = summarization_chain.run(text=text)\n",
    "\n",
    "summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8684cf-e204-4507-b098-1b64ba89c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_template = \"Translate the following text from {source_language} to {target_language}: {text}\"\n",
    "translation_prompt = PromptTemplate(input_variables=[\"source_language\", \"target_language\", \"text\"], template=translation_template)\n",
    "translation_chain = LLMChain(llm=llm, prompt=translation_prompt)\n",
    "source_language = \"English\"\n",
    "target_language = \"Spanish\"\n",
    "text = \"I don't know what your name is or where do you live or anything whatsoever.\"\n",
    "translated_text = translation_chain.predict(source_language=source_language, target_language=target_language, text=text)\n",
    "translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c394197b-32ae-4930-81eb-fa5103d60cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "template = \"You are an assistant that helps users find information about movies.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"Find information about the movie {movie_title}.\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "response = chat(chat_prompt.format_prompt(movie_title=\"Inception\").to_messages())\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0f54bb-b4ab-4a31-9b1d-d17259f54b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= chat_prompt.format_prompt(movie_title=\"Inception\").to_messages()\n",
    "messages.append(response)\n",
    "prompt= HumanMessage(content= \"Can you also recommend movies similar to this?\")\n",
    "messages.append(prompt)\n",
    "response= chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e05ca0d-5dde-487c-9d26-6d5018138c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize language model\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "\n",
    "# Load the summarization chain\n",
    "summarize_chain = load_summarize_chain(llm)\n",
    "\n",
    "# Load the document using PyPDFLoader\n",
    "document_loader = PyPDFLoader(file_path=\"/home/jupyter/self_learning/Langchain/code/Resume_Naquib_Alam.pdf\")\n",
    "document = document_loader.load()\n",
    "\n",
    "# Summarize the document\n",
    "summary = summarize_chain(document)\n",
    "print(summary['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc108ed-05d8-48c4-ba01-e6589389fd34",
   "metadata": {},
   "source": [
    "## Build a news Article Summariser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9b1643-49ad-4777-bdfe-399e55bf9f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from newspaper import Article\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "}\n",
    "\n",
    "article_url = \"https://www.artificialintelligence-news.com/2022/01/25/meta-claims-new-ai-supercomputer-will-set-records/\"\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "try:\n",
    "    response = session.get(article_url, headers=headers, timeout=10)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        article = Article(article_url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        print(f\"Title: {article.title}\")\n",
    "        print(f\"Text: {article.text}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Failed to fetch article at {article_url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while fetching article at {article_url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27217a4e-c373-4870-8935-2e112e03f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get the article data from the scraping part\n",
    "article_title = article.title\n",
    "article_text = article.text\n",
    "\n",
    "# prepare template for prompt\n",
    "template = \"\"\"You are a very good assistant that summarizes online articles.\n",
    "\n",
    "Here's the article you want to summarize.\n",
    "\n",
    "==================\n",
    "Title: {article_title}\n",
    "\n",
    "{article_text}\n",
    "==================\n",
    "\n",
    "Write a summary of the previous article.\n",
    "\"\"\"\n",
    "\n",
    "prompt = template.format(article_title=article.title, article_text=article.text)\n",
    "\n",
    "messages = [HumanMessage(content=prompt)]\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "summary= chat(messages)\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fcbfd9-7829-424d-b125-caef740fe5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare template for prompt\n",
    "template = \"\"\"You are a very good assistant that summarizes online articles.\n",
    "\n",
    "Here's the article you want to summarize.\n",
    "\n",
    "==================\n",
    "Title: {article_title}\n",
    "\n",
    "{article_text}\n",
    "==================\n",
    "\n",
    "Write a summary of the previous article in a bulleted list format.\n",
    "\"\"\"\n",
    "\n",
    "prompt = template.format(article_title=article.title, article_text=article.text)\n",
    "\n",
    "messages = [HumanMessage(content=prompt)]\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "summary= chat(messages)\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf53e3e-62c9-4e9e-9dbb-edf22bab8a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare template for prompt\n",
    "template = \"\"\"You are a very good assistant that summarizes online articles.\n",
    "\n",
    "Here's the article you want to summarize.\n",
    "\n",
    "==================\n",
    "Title: {article_title}\n",
    "\n",
    "{article_text}\n",
    "==================\n",
    "\n",
    "Write a summary of the previous article in a bulleted list format, in Spanish.\n",
    "\"\"\"\n",
    "\n",
    "prompt = template.format(article_title=article.title, article_text=article.text)\n",
    "\n",
    "messages = [HumanMessage(content=prompt)]\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "summary= chat(messages)\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c5e0b9-8ea4-459d-9ad8-3af25d95e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get the article data from the scraping part\n",
    "article_title = article.title\n",
    "article_text = article.text\n",
    "\n",
    "# prepare template for prompt\n",
    "template = \"\"\"\n",
    "As an advanced AI, you've been tasked to summarize online articles into bulleted points. Here are a few examples of how you've done this in the past:\n",
    "\n",
    "Example 1:\n",
    "Original Article: 'The Effects of Climate Change\n",
    "Summary:\n",
    "- Climate change is causing a rise in global temperatures.\n",
    "- This leads to melting ice caps and rising sea levels.\n",
    "- Resulting in more frequent and severe weather conditions.\n",
    "\n",
    "Example 2:\n",
    "Original Article: 'The Evolution of Artificial Intelligence\n",
    "Summary:\n",
    "- Artificial Intelligence (AI) has developed significantly over the past decade.\n",
    "- AI is now used in multiple fields such as healthcare, finance, and transportation.\n",
    "- The future of AI is promising but requires careful regulation.\n",
    "\n",
    "Now, here's the article you need to summarize:\n",
    "\n",
    "==================\n",
    "Title: {article_title}\n",
    "\n",
    "{article_text}\n",
    "==================\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# create output parser class\n",
    "class ArticleSummary(BaseModel):\n",
    "    title: str = Field(description=\"Title of the article\")\n",
    "    summary: List[str] = Field(description=\"Bulleted list summary of the article\")\n",
    "\n",
    "    # validating whether the generated summary has at least three lines\n",
    "    @validator('summary', allow_reuse=True)\n",
    "    def has_three_or_more_lines(cls, list_of_lines):\n",
    "        if len(list_of_lines) < 3:\n",
    "            raise ValueError(\"Generated summary has less than three bullet points!\")\n",
    "        return list_of_lines\n",
    "\n",
    "# set up output parser\n",
    "parser = PydanticOutputParser(pydantic_object=ArticleSummary)\n",
    "\n",
    "# template = \"\"\"\n",
    "# You are a very good assistant that summarizes online articles.\n",
    "\n",
    "# Here's the article you want to summarize.\n",
    "\n",
    "# ==================\n",
    "# Title: {article_title}\n",
    "\n",
    "# {article_text}\n",
    "# ==================\n",
    "\n",
    "# {format_instructions}\n",
    "# \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"article_title\", \"article_text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Format the prompt using the article title and text obtained from scraping\n",
    "formatted_prompt = prompt.format_prompt(article_title=article_title, article_text=article_text)\n",
    "\n",
    "\n",
    "# instantiate model class\n",
    "model = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.0)\n",
    "\n",
    "# Use the model to generate a summary\n",
    "output = model(formatted_prompt.to_string())\n",
    "\n",
    "# Parse the output into the Pydantic model\n",
    "parsed_output = parser.parse(output.split(\"\\\"]}\")[0] + \"\\\"]}\")\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d94193-6b13-49e2-b283-d897ac73d473",
   "metadata": {},
   "source": [
    "## Using the open-source GPT4ALL models locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "353c5d04-fc0f-4cef-83d1-b95d60d7127e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Response' object has no attribute 'download'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m url\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/blob/main/llama-2-13b-chat.ggmlv3.q2_K.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# send a GET request to the URL to download the file.\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# open the file in binary mode and write the contents of the response\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# to it in chunks.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(local_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Response' object has no attribute 'download'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "local_path = './models/llama-2-13b-chat.ggmlv3.q2_K.bin'\n",
    "Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# url = 'https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized-ggml.bin'\n",
    "url= \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/blob/main/llama-2-13b-chat.ggmlv3.q2_K.bin\"\n",
    "\n",
    "# send a GET request to the URL to download the file.\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "# open the file in binary mode and write the contents of the response\n",
    "# to it in chunks.\n",
    "with open(local_path, 'wb') as f:\n",
    "    for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
    "        if chunk:\n",
    "            f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f360e701-ede6-4a1c-a7c1-5635c69a06a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gguf_init_from_file: invalid magic characters 'tjgg'\n",
      "gguf_init_from_file: invalid magic characters 'tjgg'\n",
      "load_gguf: gguf_init_from_file failed\n",
      "magic_match: unsupported model architecture: \n",
      "gguf_init_from_file: invalid magic characters 'tjgg'\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for GPT4All\n__root__\n  Unable to instantiate model: Model format not supported (no matching implementation found) (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(template\u001b[38;5;241m=\u001b[39mtemplate, input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m llm \u001b[38;5;241m=\u001b[39m GPT4All(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/ggml-model-q4_0.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(prompt\u001b[38;5;241m=\u001b[39mprompt, llm\u001b[38;5;241m=\u001b[39mllm)\n\u001b[1;32m     10\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat happens when it rains somewhere?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/libraries/llm_exp_1/lib/python3.11/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/libraries/llm_exp_1/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for GPT4All\n__root__\n  Unable to instantiate model: Model format not supported (no matching implementation found) (type=value_error)"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = GPT4All(model=\"./models/ggml-model-q4_0.bin\", verbose=True)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What happens when it rains somewhere?\"\n",
    "llm_chain.run(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8765ff96-b080-4c95-a9da-f5eeeb5156bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's answer in two sentence while being funny.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What happens when it rains somewhere?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c349a1c-01e2-41a3-8c99-73e0d5059033",
   "metadata": {},
   "source": [
    "## What other open-source models we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3399d1e7-4dae-470f-a68d-1893ece01fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Hub LLM\n",
    "dolly3b_llm = HuggingFaceHub(repo_id='google/flan-t5-xl', model_kwargs={'temperature':0.1})\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's answer in two sentence while being funny.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=dolly3b_llm)\n",
    "\n",
    "question = \"What happens when it rains somewhere?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56a6c7-889b-47eb-998e-bf61fdafcf6e",
   "metadata": {},
   "source": [
    "https://learn.activeloop.ai/courses/take/langchain/multimedia/46317676-what-other-models-can-we-use-popular-llm-models-compared\n",
    "__Various models to use with HuggingFace hub__: \\\n",
    "* repo_id='google/flan-t5-large/xl/xxl', model_kwargs={'temperature':0}\n",
    "* repo_id='databricks/dolly-v2-12b/3b/7b', model_kwargs={'temperature':0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c49b3a8-1b7d-49bc-85c4-4a6f6b7b1c7a",
   "metadata": {},
   "source": [
    "## Using open-source models in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d06bd-0640-4e5c-a5fa-3f9cf303a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "generate_text = pipeline(model=\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.bfloat16,\n",
    "                         trust_remote_code=True, device_map=\"auto\", return_full_text=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87fc835-72b1-4d69-a16d-ff60995ad918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# template for an instrution with no input\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\"],\n",
    "    template=\"{instruction}\")\n",
    "\n",
    "# template for an instruction with input\n",
    "prompt_with_context = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"context\"],\n",
    "    template=\"{instruction}\\n\\nInput:\\n{context}\")\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "llm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)\n",
    "llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\n",
    "print(llm_chain.predict(instruction=\"Explain to me the difference between nuclear fission and fusion.\").lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c46d9b8-fdad-469c-9d28-c1f3b8062512",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's answer in two sentence while being funny.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "question = \"What happens when it rains somewhere?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e5ea3e-a453-4e6f-acf2-1638e63a2a61",
   "metadata": {},
   "source": [
    "# Keeping Knowledge organised with indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21299898-6861-43fe-a040-cd010a5427f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text to write to a local file\n",
    "# taken from https://www.theverge.com/2023/3/14/23639313/google-ai-language-model-palm-api-challenge-openai\n",
    "text = \"\"\"Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\n",
    "Google is offering developers access to one of its most advanced AI language models: PaLM.\n",
    "The search giant is launching an API for PaLM alongside a number of AI enterprise tools\n",
    "it says will help businesses “generate text, images, code, videos, audio, and more from\n",
    "simple natural language prompts.”\n",
    "\n",
    "PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\n",
    "Meta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\n",
    "PaLM is a flexible system that can potentially carry out all sorts of text generation and\n",
    "editing tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\n",
    "example, or you could use it for tasks like summarizing text or even writing code.\n",
    "(It’s similar to features Google also announced today for its Workspace apps like Google\n",
    "Docs and Gmail.)\n",
    "\"\"\"\n",
    "\n",
    "# write text to local file\n",
    "with open(\"my_file.txt\", \"w\") as file:\n",
    "    file.write(text)\n",
    "\n",
    "# use TextLoader to load text from local file\n",
    "loader = TextLoader(\"my_file.txt\")\n",
    "docs_from_file = loader.load()\n",
    "\n",
    "print(len(docs_from_file))\n",
    "\n",
    "# create a text splitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "\n",
    "# split documents into chunks\n",
    "docs = text_splitter.split_documents(docs_from_file)\n",
    "\n",
    "print(len(docs))\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "chromadb_client = chromadb.Client()\n",
    "db_openai = Chroma.from_documents(\n",
    "    docs, embeddings, client=chromadb_client, collection_name=\"tmp_collection_1\"\n",
    ")\n",
    "\n",
    "print(\"There are\", db_openai._collection.count(), \"in the collection\")\n",
    "\n",
    "retriever = db_openai.as_retriever()\n",
    "model= OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "# create a retrieval chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm= model, chain_type=\"stuff\", retriever=retriever)\n",
    "query = \"How Google plans to challenge OpenAI?\"\n",
    "response = qa_chain.run(query)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f8f541-8034-42ca-920d-31dc527a057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create compressor for the retriever\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever( base_compressor=compressor, base_retriever=retriever)\n",
    "\n",
    "# retrieving compressed documents\n",
    "retrieved_docs = compression_retriever.get_relevant_documents(\"How Google plans to challenge OpenAI?\")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c51967b-67d5-4cc5-bfe6-9f477b3dd694",
   "metadata": {},
   "source": [
    "## Exploring the world of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c56ce2-222e-4c7d-a080-7e5e3cf2f2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OpenAI Embedding\n",
    "# Define the documents\n",
    "print(\"=========OpenAI Embeddings===========\")\n",
    "documents = [\n",
    "    \"The cat is on the mat.\",\n",
    "    \"There is a cat on the mat.\",\n",
    "    \"The dog is in the yard.\",\n",
    "    \"There is a dog in the yard.\",\n",
    "]\n",
    "\n",
    "# Initialize the OpenAIEmbeddings instance\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Generate embeddings for the documents\n",
    "document_embeddings = embeddings.embed_documents(documents)\n",
    "\n",
    "# Perform a similarity search for a given query\n",
    "query = \"A cat is sitting on a mat.\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "# Calculate similarity scores\n",
    "similarity_scores = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "\n",
    "# Find the most similar document\n",
    "most_similar_index = np.argmax(similarity_scores)\n",
    "most_similar_document = documents[most_similar_index]\n",
    "\n",
    "print(f\"Most similar document to the query '{query}':\")\n",
    "print(most_similar_document)\n",
    "\n",
    "\n",
    "## HuggingFace Embedding\n",
    "print(\"=========HuggingFace Embeddings===========\")\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cuda:0'}\n",
    "hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "document_embeddings = hf.embed_documents(documents)\n",
    "query_embedding = hf.embed_query(query)\n",
    "\n",
    "# Calculate similarity scores\n",
    "similarity_scores = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "print(similarity_scores)\n",
    "# Find the most similar document\n",
    "most_similar_index = np.argmax(similarity_scores)\n",
    "most_similar_document = documents[most_similar_index]\n",
    "\n",
    "print(f\"Most similar document to the query '{query}':\")\n",
    "print(most_similar_document)\n",
    "\n",
    "## Cohere Embedding\n",
    "print(\"=========Cohere Embeddings===========\")\n",
    "\n",
    "# Initialize the CohereEmbeddings object\n",
    "cohere = CohereEmbeddings(model=\"embed-multilingual-v2.0\")\n",
    "\n",
    "# Define a list of texts\n",
    "docs= [\n",
    "    \"Hello from Cohere!\", \n",
    "    \"مرحبًا من كوهير!\", \n",
    "    \"Hallo von Cohere!\",  \n",
    "    \"Bonjour de Cohere!\", \n",
    "    \"¡Hola desde Cohere!\", \n",
    "    \"Olá do Cohere!\",  \n",
    "    \"Ciao da Cohere!\", \n",
    "    \"您好，来自 Cohere！\", \n",
    "    \"कोहेरे से नमस्ते!\",\n",
    "    \"Paris is a very beautiful city.\",\n",
    "    \"París es una ciudad muy hermosa.\",\n",
    "    \"Paris est une très belle ville.\",\n",
    "    \"पेरिस बहुत खूबसूरत शहर है.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for the texts\n",
    "document_embeddings = cohere.embed_documents(docs)\n",
    "query = [\"Hello from Cohere!\", \"पेरिस बहुत खूबसूरत शहर है.\",  \"您好，来自 Cohere！\"]\n",
    "query_embedding = cohere.embed_query(query[1])\n",
    "# Calculate similarity scores\n",
    "similarity_scores = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "sorted_similarity_scores_idx = np.argsort(-similarity_scores)[:5]\n",
    "topk_docs= [docs[i] for i in sorted_similarity_scores_idx]\n",
    "print(topk_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef6b94-ab6d-4dea-a285-b5ba2f3903dc",
   "metadata": {},
   "source": [
    "## Build a customer support chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228f2221-d65a-4844-b51c-ed9bf3bc71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import SeleniumURLLoader\n",
    "# we'll use information from the following articles\n",
    "urls = ['https://beebom.com/what-is-nft-explained/',\n",
    "        'https://beebom.com/how-delete-spotify-account/',\n",
    "        'https://beebom.com/how-download-gif-twitter/',\n",
    "        'https://beebom.com/how-use-chatgpt-linux-terminal/',\n",
    "        'https://beebom.com/how-delete-spotify-account/',\n",
    "        'https://beebom.com/how-save-instagram-story-with-music/',\n",
    "        'https://beebom.com/how-install-pip-windows/',\n",
    "        'https://beebom.com/how-check-disk-usage-linux/']\n",
    "\n",
    "# use the selenium scraper to load the documents\n",
    "loader = SeleniumURLLoader(urls=urls, browser= 'firefox')\n",
    "docs_not_splitted = loader.load()\n",
    "\n",
    "# we split the documents into smaller chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(docs_not_splitted)\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "chromadb_client = chromadb.Client()\n",
    "db_openai = Chroma.from_documents(\n",
    "    docs, embeddings, client=chromadb_client, collection_name=\"tmp_collection_2\"\n",
    ")\n",
    "\n",
    "print(\"There are\", db_openai._collection.count(), \"in the collection\")\n",
    "\n",
    "\n",
    "# let's write a prompt for a customer support chatbot that\n",
    "# answer questions using information extracted from our db\n",
    "template = \"\"\"You are an exceptional customer support chatbot that gently answer questions.\n",
    "\n",
    "You know the following context information.\n",
    "\n",
    "{chunks_formatted}\n",
    "\n",
    "Answer to the following question from a customer. Use only information from the previous context information. Do not invent stuff.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chunks_formatted\", \"query\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# the full pipeline\n",
    "\n",
    "# user question\n",
    "query = \"How to check disk usage in linux?\"\n",
    "\n",
    "# retrieve relevant chunks\n",
    "docs = db_openai.similarity_search(query)\n",
    "retrieved_chunks = [doc.page_content for doc in docs]\n",
    "print(len(retrieved_chunks))\n",
    "# format the prompt\n",
    "chunks_formatted = \"\\n\\n\".join(retrieved_chunks)\n",
    "prompt_formatted = prompt.format(chunks_formatted=chunks_formatted, query=query)\n",
    "\n",
    "# generate answer\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "answer = llm(prompt_formatted)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade6fab3-d220-4566-9a3d-6a7680acd820",
   "metadata": {},
   "source": [
    "# Combining components together with Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd566b6-ae7d-4048-a585-32ef2c44e461",
   "metadata": {},
   "source": [
    "## Chains and why they are used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310b5ad8-6cc2-4468-8383-9a7bb5114eae",
   "metadata": {},
   "source": [
    "### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60418974-324e-40f0-861a-c48e39b7e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "# poet\n",
    "poet_template= \"\"\"You are an American poet, your job is to come up with\\\n",
    "poems based on a given theme.\n",
    "\n",
    "Here is the theme you have been asked to generate a poem on:\n",
    "{input}\\\n",
    "\"\"\"\n",
    "\n",
    "poet_prompt_template = PromptTemplate(input_variables=[\"input\"], template=poet_template)\n",
    "\n",
    "# creating the poet chain\n",
    "poet_chain = LLMChain(llm=llm, output_key=\"poem\", prompt=poet_prompt_template)\n",
    "\n",
    "# critic\n",
    "critic_template= \"\"\"You are a critic of poems, you are tasked\\\n",
    "to inspect the themes of poems. Identify whether the poem includes romantic expressions or descriptions of nature.\n",
    "\n",
    "Your response should be in the following format, as a Python Dictionary.\n",
    "poem: this should be the poem you received \n",
    "Romantic_expressions: True or False\n",
    "Nature_descriptions: True or False\n",
    "\n",
    "Here is the poem submitted to you:\n",
    "{poem}\\\n",
    "\"\"\"\n",
    "\n",
    "critic_prompt_template = PromptTemplate(input_variables=[\"poem\"], template=critic_template)\n",
    "\n",
    "# creating the critic chain\n",
    "critic_chain = LLMChain(llm=llm, output_key=\"critic_verified\", prompt=critic_prompt_template)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[poet_chain, critic_chain])\n",
    "\n",
    "# Run the poet and critic chain with a specific theme\n",
    "theme= \"the beauty of nature\"\n",
    "review = overall_chain.run(theme)\n",
    "\n",
    "# Print the review to see the critic's evaluation\n",
    "print(review)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b37a7d5-f649-444b-ad9e-c523e592b4f8",
   "metadata": {},
   "source": [
    "### Custom Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dfa390-98ad-4b76-b059-70e8908e8855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class ConcatenateChain(Chain):\n",
    "    chain_1: LLMChain\n",
    "    chain_2: LLMChain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        # Union of the input keys of the two chains.\n",
    "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
    "        return list(all_input_vars)\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['concat_output']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        output_1 = self.chain_1.run(inputs)\n",
    "        output_2 = self.chain_2.run(inputs)\n",
    "        return {'concat_output': output_1 + output_2}\n",
    "\n",
    "prompt_1 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is the meaning of the following word '{word}'?\",\n",
    ")\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "\n",
    "prompt_2 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is a word to replace the following: {word}?\",\n",
    ")\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
    "\n",
    "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
    "concat_output = concat_chain.run(\"artificial\")\n",
    "print(f\"Concatenated output:\\n{concat_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f89e146-3962-4556-863f-e41964aa3b89",
   "metadata": {},
   "source": [
    "## Create a Youtube video summarizer using Whisper and Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c174f9-5558-4388-b00e-efec12d67271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "import whisper\n",
    "\n",
    "# #Download youtube video\n",
    "# # url = \"https://www.youtube.com/watch?v=mBjPyte2ZZo\"\n",
    "# # url= \"https://youtu.be/IuL-l2L_8Rk\"\n",
    "# url= \"https://www.youtube.com/watch?v=QHfuiT1Y7Hw\"\n",
    "# savepath = '/home/jupyter/self_learning/Langchain/data/'\n",
    "# YouTube(url).streams.filter(file_extension='mp4', res=\"144p\").first().download(savepath)\n",
    "# # YouTube(url).streams.first().download(savepath)\n",
    "\n",
    "\n",
    "#load Whisper model and generate transcription of the video\n",
    "savepath = '/home/jupyter/self_learning/Langchain/data/Yann LeCun on How to Fill the Gaps in Large Language Models.mp4'\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(savepath)\n",
    "print(result['text'])\n",
    "\n",
    "with open ('/home/jupyter/self_learning/Langchain/data/yann_lecun_transcription.txt', 'w') as file:  \n",
    "    file.write(result['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4921471a-ce5d-4936-8027-e920e646d226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Craig Smith interviews Jan LeCoon, a pioneer in deep learning and self-supervised learning,\n",
      "discussing the limitations of large language models and Jan's new joint embedding predictive\n",
      "architecture. Jan also shares his thoughts on consciousness and the potential for AI systems to\n",
      "exhibit it. They discuss the use of self-supervised learning in natural language processing and its\n",
      "potential for training neural networks to predict missing information in videos. The article also\n",
      "explores the need for machines to learn about the world through observation and accumulate common\n",
      "sense knowledge. The author proposes using self-supervised learning to teach machines how to predict\n",
      "the future and plan actions accordingly. They also discuss the limitations of current language\n",
      "models in understanding underlying reality and the potential for a joint embedding architecture to\n",
      "address this. The article also delves into the speaker's work on developing new learning procedures\n",
      "for complex neural networks and their interest in understanding the principles of intelligence. They\n",
      "also discuss the potential for AI systems to exhibit sentience and consciousness through a world\n",
      "model. The article touches on the use of neural networks and transformer architectures in\n",
      "understanding brain activity and language processing, as well as the importance of collaboration and\n",
      "contributions from others in making ideas successful. The speaker also discusses their love for\n",
      "coding and the challenges of being listened to and making claims in the field as their fame has\n",
      "grown\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
    ")\n",
    "\n",
    "with open('/home/jupyter/self_learning/Langchain/data/yann_lecun_transcription.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "texts = text_splitter.split_text(text)\n",
    "docs = [Document(page_content=t) for t in texts[:10]]\n",
    "\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeacdbb8-fc16-446e-952c-6cc528668644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "print(chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6957dec-4c7d-4895-9cb2-197516c9d2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "- Craig Smith interviews Jan LeCoon, a leading figure in deep learning and self-supervised learning.\n",
      "- Jan discusses his new joint embedding predictive architecture and its potential to fill gaps in large language models.\n",
      "- He also talks about his theory of consciousness and the potential for AI systems to exhibit conscious features.\n",
      "- Jan explains the limitations of large language models and the need for machines to learn predictive world models.\n",
      "- He suggests that incorporating self-supervised learning and planning capabilities into large language models could improve their performance.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"Write a concise bullet point summary of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
    "\n",
    "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template, \n",
    "                        input_variables=[\"text\"])\n",
    "\n",
    "chain = load_summarize_chain(llm, \n",
    "                             chain_type=\"stuff\", \n",
    "                             prompt=BULLET_POINT_PROMPT)\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "\n",
    "wrapped_text = textwrap.fill(output_summary, \n",
    "                             width=1000,\n",
    "                             break_long_words=False,\n",
    "                             replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a82f0452-ef8c-43c7-876b-c930a24501c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Craig Smith interviews Jan LeCoon, a pioneer in deep learning and self-supervised learning. They\n",
      "discuss the limitations of large language models and Jan's new joint embedding predictive\n",
      "architecture. Jan also shares his thoughts on consciousness and the potential for AI systems to\n",
      "exhibit it. Jan is a professor at New York University and the Guarante Institute in the Centre for\n",
      "Data Science, as well as the chief AI scientist at Fair, a fundamental AI research lab. They also\n",
      "touch on the use of transformer architectures in self-supervised learning and its impact on natural\n",
      "language processing. Jan explains that self-supervised learning involves training a large neural net\n",
      "to predict missing words in a piece of text, resulting in improved text representations for\n",
      "downstream tasks such as translation and content moderation. This technique has seen a revolution in\n",
      "the past few years and is now widely used in practical applications. However, Jan's recent position\n",
      "paper proposes using self-supervised learning to train machines to predict the continuation of a\n",
      "video or the consequences of an action, in order to create predictive world models. This approach\n",
      "has shown promise in the robotics community, where it is necessary for intelligent agents to plan\n",
      "complex sequences of actions to achieve a goal. Jan also suggests that this approach could\n",
      "potentially solve some of the problems observed with large language models, as\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4fa2c2-c46a-4a3e-ad22-dde5d6e04197",
   "metadata": {},
   "source": [
    "### Using RAG framework with chromadb for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f38f9dda-4467-4375-b578-322465f25379",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Combine both transcript files in one text file\n",
    "path= \"/home/jupyter/self_learning/Langchain/data/\"\n",
    "filenames = ['yann_lecun_transcription.txt', 'thecottagefairy_transcription.txt']\n",
    "with open(path+'yannlecun_thecottagefairy_comb_transcription.txt', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(path+fname) as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d249eb59-cbfd-42b2-a99b-2fd1af61d8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 60\n"
     ]
    }
   ],
   "source": [
    "with open('/home/jupyter/self_learning/Langchain/data/yannlecun_thecottagefairy_comb_transcription.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Split the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
    "    )\n",
    "texts = text_splitter.split_text(text)\n",
    "docs = [Document(page_content=t) for t in texts]\n",
    "print(len(texts), len(docs))\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9860ba5f-6268-41b0-b40f-5bd91ce14aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60 in the collection\n"
     ]
    }
   ],
   "source": [
    "chromadb_client = chromadb.Client()\n",
    "db_openai = Chroma.from_documents(\n",
    "    docs, embeddings, client=chromadb_client, collection_name=\"collection_transcript\"\n",
    ")\n",
    "\n",
    "print(\"There are\", db_openai._collection.count(), \"in the collection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94f77539-89c7-419f-8204-a501a2dfb2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yann Lecun is a professor at New York University and at the Guarante Institute in the Centre for Data Science. He is also the chief AI scientist at Fair, which is the fundamental AI research lab at Facebook. He is a seminal figure in deep learning development and a long time proponent of self-supervised learning.\n"
     ]
    }
   ],
   "source": [
    "retriever = db_openai.as_retriever()\n",
    "# retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "retriever.search_kwargs['k'] = 4\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of transcripts from a video to answer the question. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=retriever,\n",
    "                                 chain_type_kwargs=chain_type_kwargs)\n",
    "\n",
    "# print( qa.run(\"Summarize the mentions of google according to their AI program\") )\n",
    "# print( qa.run(\"what's lacking in large language models?\") )\n",
    "print( qa.run(\"Who is Yann Lecun?\") )\n",
    "# print( qa.run(\"What is the reason for Will Smith's popularity?\") )\n",
    "# print( qa.run(\"Who is Deepika Padukone?\") ) ##How to restrict answers outside the transcript.\n",
    "# print( qa.run(\"What are different ways of self-care?\") )\n",
    "# print( qa.run(\"What kind of videos she makes?\") )\n",
    "# print( qa.run(\"When did she start this youtube channel?\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866721d6-a36e-43d2-917d-863a6e8f8522",
   "metadata": {},
   "source": [
    "### How to restrict answers outside the transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e92a3c1-70e4-4262-b06c-e0c3a1a103be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6bcdb75-1e92-434b-b0e7-9fad8e925335",
   "metadata": {},
   "source": [
    "## Rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f56eb54b-949d-4f34-a16c-4ca97f43608e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_mistralai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_mistralai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatMistralAI\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_mistralai'"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f47bb8-2ba6-48cc-a17f-f688b6be3dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
