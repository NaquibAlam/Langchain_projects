{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f822d3cb-ee14-4fb6-9ce9-111189861857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from typing import List, Optional, Tuple, TypedDict\n",
    "\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c2142c-3057-4432-89fa-cb62b6d786ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7348fe60-dfb5-4546-920b-f7adca32d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jupyter/self_learning/Langchain/code/llama.cpp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa1b9028-381f-4f59-af5b-684dae2ec429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c79b63-7804-477f-b843-2d7da31963ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"\"\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"]= \"\"\n",
    "os.environ[\"GOOGLE_CSE_ID\"]= \"\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]= \"\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"\"\n",
    "os.environ[\"WOLFRAM_ALPHA_APPID\"] = \"\"\n",
    "os.environ[\"SERPAPI_API_KEY\"]= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eb70d66-53f8-4f8a-b81c-426d722ef9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file scrapes disney songs + lyrics from \"https://www.disneyclips.com/lyrics/\"\n",
    "\"\"\"\n",
    "\n",
    "URL = \"https://www.disneyclips.com/lyrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c729c2f4-baa1-4d12-9cb2-b231f47c20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_lyrics_names_and_urls_from_movie_url(\n",
    "    movie_name: str, url: str, session: aiohttp.ClientSession\n",
    ") -> List[Tuple[str, str]]:\n",
    "    async with session.get(url) as response:\n",
    "        html = await response.text()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        table = soup.find(\"table\", {\"class\": \"songs\"})\n",
    "        names_and_urls = []\n",
    "        if table:\n",
    "            links = table.find_all(\"a\")\n",
    "            names_and_urls = []\n",
    "            for link in links:\n",
    "                names_and_urls.append(\n",
    "                    (movie_name, link.text, f\"{URL}/{link.get('href')}\")\n",
    "                )\n",
    "        return names_and_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ec4d5-f2f1-46e8-9b2d-fd3c3fb2e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_lyric_from_lyric_url(\n",
    "    movie_name: str, lyric_name: str, url: str, session: aiohttp.ClientSession\n",
    ") -> str:\n",
    "    async with session.get(url) as response:\n",
    "        html = await response.text()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        div = soup.find(\"div\", {\"id\": \"cnt\"}).find(\"div\", {\"class\": \"main\"})\n",
    "        paragraphs = div.find_all(\"p\")\n",
    "        text = \"\"\n",
    "        # first <p> has the lyric\n",
    "        p = paragraphs[0]\n",
    "        for br in p.find_all(\"br\"):\n",
    "            br.replace_with(\". \")\n",
    "        for span in p.find_all(\"span\"):\n",
    "            span.decompose()\n",
    "        text += p.text\n",
    "\n",
    "        return (movie_name, lyric_name, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53337780-4d41-4971-8966-3debd84da40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_movie_names_and_urls(\n",
    "    session: aiohttp.ClientSession,\n",
    ") -> List[Tuple[str, str]]:\n",
    "    async with session.get(URL) as response:\n",
    "        html = await response.text()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = (\n",
    "            soup.find(\"div\", {\"id\": \"cnt\"}).find(\"div\", {\"class\": \"main\"}).find_all(\"a\")\n",
    "        )\n",
    "        movie_names_and_urls = [\n",
    "            (link.text, f\"{URL}/{link.get('href')}\") for link in links\n",
    "        ]\n",
    "        return movie_names_and_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1827ba-f887-4399-b7ad-cae62f331454",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_disney_lyrics():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        data = await get_movie_names_and_urls(session)\n",
    "        data = await asyncio.gather(\n",
    "            *[\n",
    "                asyncio.create_task(\n",
    "                    get_lyrics_names_and_urls_from_movie_url(*el, session)\n",
    "                )\n",
    "                for el in data\n",
    "            ]\n",
    "        )\n",
    "        data = await asyncio.gather(\n",
    "            *[\n",
    "                asyncio.create_task(get_lyric_from_lyric_url(*data, session))\n",
    "                for data in chain(*data)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        result = defaultdict(list)\n",
    "\n",
    "        for movie_name, lyric_name, lyric_text in data:\n",
    "            result[movie_name].append({\"name\": lyric_name, \"text\": lyric_text})\n",
    "\n",
    "        with open(\"/home/jupyter/self_learning/Langchain/code/fairytaleDJ/data/lyrics.json\", \"w\") as f:\n",
    "            json.dump(result, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4136372c-424e-422c-854d-dca29a7a4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(scrape_disney_lyrics())\n",
    "# asyncio.run(scrape_disney_lyrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe72753-13f2-493b-8334-f5a745b966a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "import json\n",
    "import os\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "from langchain.embeddings import CohereEmbeddings\n",
    "# MODEL_ID = \"text-embedding-ada-002\"\n",
    "EMBED_MODEL = CohereEmbeddings(model=\"embed-multilingual-v2.0\")\n",
    "COLLECTION_NAME = \"disney-lyrics\"\n",
    "# DATASET_ID = \"disney-lyrics-emotions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e4befd-a942-481f-af8c-87c3a8dfcaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(chromadb_collection_name: str, json_filepath: str):\n",
    "    with open(json_filepath, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "\n",
    "    for movie, lyrics in data.items():\n",
    "        for lyric in lyrics:\n",
    "            texts.append(lyric[\"text\"])\n",
    "            metadatas.append(\n",
    "                {\n",
    "                    \"movie\": movie,\n",
    "                    \"name\": lyric[\"name\"],\n",
    "                    \"embed_url\": lyric[\"text\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "#     embeddings = OpenAIEmbeddings(model=MODEL_ID)\n",
    "    client = chromadb.Client()\n",
    "    db = Chroma.from_texts(texts, EMBED_MODEL, client=client, metadatas=metadatas, collection_name=chromadb_collection_name)\n",
    "    print(\"There are\", db._collection.count(), \"in the collection\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df07d4-895e-423e-a311-a948ad7dacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_db(dataset_path: str, *args, **kwargs) -> DeepLake:\n",
    "#     db = DeepLake(dataset_path, *args, **kwargs)\n",
    "#     return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec0f7e-dc44-4cdb-9b1d-a7f9d76a26f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#     dataset_path = f\"hub://{os.environ['ACTIVELOOP_ORG_ID']}/{DATASET_ID}\"\n",
    "    create_db(COLLECTION_NAME, \"/home/jupyter/self_learning/Langchain/code/fairytaleDJ/data/lyrics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893209b3-ac21-4827-96c8-3fea52b3821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client()\n",
    "collection = client.get_collection(name=COLLECTION_NAME, embedding_function= EMBED_MODEL)\n",
    "print(\"There are\", collection.count(), \"in the collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9756544-b4f0-479b-849a-a8d45061a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.get(include=['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08495aa4-e679-44f4-9c4b-5f5217dacfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# This script will keep only the songs that are in the Spotify \"Disney Hits\" playlist\n",
    "# \"\"\"\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "# import json\n",
    "# from collections import defaultdict\n",
    "\n",
    "# import spotipy\n",
    "# from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "# name = \"Disney hits\"\n",
    "\n",
    "# spotify = spotipy.Spotify(auth_manager=SpotifyClientCredentials())\n",
    "# results = spotify.search(q=\"playlist:\" + name, type=\"playlist\", limit=5)\n",
    "# items = results[\"playlists\"][\"items\"]\n",
    "\n",
    "# uri = \"spotify:playlist:37i9dQZF1DX8C9xQcOrE6T\"\n",
    "# playlist = spotify.playlist(uri)\n",
    "\n",
    "# with open(\"/home/jupyter/self_learning/Langchain/code/fairytaleDJ/data/lyrics.json\", \"r\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# spotify_tracks = {}\n",
    "\n",
    "# for item in playlist[\"tracks\"][\"items\"]:\n",
    "#     track = item[\"track\"]\n",
    "#     track_name = track[\"name\"].lower().split(\"-\")[0].strip()\n",
    "#     print(track_name)\n",
    "#     spotify_tracks[track_name] = {\n",
    "#         \"id\": track[\"id\"],\n",
    "#         \"embed_url\": f\"https://open.spotify.com/embed/track/{track['id']}?utm_source=generator\",\n",
    "#     }\n",
    "\n",
    "# # here we add only songs that are in the Disney spotify playlist\n",
    "\n",
    "# data_filtered = defaultdict(list)\n",
    "# tot = 0\n",
    "# for movie, lyrics in data.items():\n",
    "#     for lyric in lyrics:\n",
    "#         name = lyric[\"name\"].lower()\n",
    "#         if name in spotify_tracks:\n",
    "#             data_filtered[movie].append(\n",
    "#                 {**lyric, **{\"embed_url\": spotify_tracks[name][\"embed_url\"]}}\n",
    "#             )\n",
    "#             tot += 1\n",
    "# print(tot)\n",
    "\n",
    "# with open(\"data/lyrics_with_spotify_url.json\", \"w\") as f:\n",
    "#     json.dump(data_filtered, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1adad15-ea3f-4c80-8fd8-4139572f6e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /home/jupyter/self_learning/Langchain/code/models/mistral-7b-instruct-v0.2.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4892.99 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32768\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  4096.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  2144.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
      "Guessed chat format: mistral-instruct\n",
      "/libraries/llm_exp_1/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cruella De Vil\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    9266.68 ms\n",
      "llama_print_timings:      sample time =      44.22 ms /    76 runs   (    0.58 ms per token,  1718.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9266.24 ms /   199 tokens (   46.56 ms per token,    21.48 tokens per second)\n",
      "llama_print_timings:        eval time =    6676.94 ms /    75 runs   (   89.03 ms per token,    11.23 tokens per second)\n",
      "llama_print_timings:       total time =   16213.85 ms /   274 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dalmatian Plantation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    9266.68 ms\n",
      "llama_print_timings:      sample time =      22.43 ms /    40 runs   (    0.56 ms per token,  1783.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3560.40 ms /    83 tokens (   42.90 ms per token,    23.31 tokens per second)\n",
      "llama_print_timings:        eval time =    3576.30 ms /    39 runs   (   91.70 ms per token,    10.91 tokens per second)\n",
      "llama_print_timings:       total time =    7275.85 ms /   122 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kanine Krunchies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    9266.68 ms\n",
      "llama_print_timings:      sample time =      25.81 ms /    44 runs   (    0.59 ms per token,  1704.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6410.05 ms /   149 tokens (   43.02 ms per token,    23.24 tokens per second)\n",
      "llama_print_timings:        eval time =    3833.21 ms /    43 runs   (   89.14 ms per token,    11.22 tokens per second)\n",
      "llama_print_timings:       total time =   10396.75 ms /   192 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I See Spots\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script takes all the songs we have and use the lyric to create a list of 8 emotions we then use to replace the lyric itself.\n",
    "This is needed to properly match user's emotions to the songs.\n",
    "\"\"\"\n",
    "from langchain_community.chat_models import ChatCohere\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"song\"],\n",
    "    template=Path(\"/home/jupyter/self_learning/Langchain/code/fairytaleDJ/data/summary_with_emotions.prompt\").read_text(),\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "# llm= ChatCohere(model=\"command\", temperature=0.7)\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/home/jupyter/self_learning/Langchain/code/models/mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=512,\n",
    "    n_ctx=32768,\n",
    "    f16_kv=True,\n",
    "#     callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "with open(\"/home/jupyter/self_learning/Langchain/code/fairytaleDJ/data/lyrics.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "new_data = defaultdict(list)\n",
    "\n",
    "for movie, songs in data.items():\n",
    "    for song in songs:\n",
    "        print(f\"{song['name']}\")\n",
    "        emotions = chain.run(song=song[\"text\"])\n",
    "        new_data[movie].append(\n",
    "            {\"name\": song[\"name\"], \"text\": emotions, \"lyrics\": song[\"text\"]}\n",
    "        )\n",
    "        with open(\"/home/jupyter/self_learning/Langchain/code/fairytaleDJ/data/emotions_with_spotify_url_mistral_7b.json\", \"w\") as f:\n",
    "            json.dump(new_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b5c13-7e42-4dc2-be2a-7dc652c55314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script takes all the songs we have and create a summary for each lyric\n",
    "\"\"\"\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"song\"],\n",
    "    template=Path(\"/home/jupyter/self_learning/Langchain/code/fairytaleDJ/data/summary.prompt\").read_text(),\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "with open(\"/home/jupyter/self_learning/Langchain/code/fairytaleDJ/data/lyrics.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "lyrics_summaries = {}\n",
    "\n",
    "for movie, lyrics in data.items():\n",
    "    for lyric in lyrics:\n",
    "        print(f\"Creating summary for {lyric['name']}\")\n",
    "        summary = chain.run(song=lyric[\"text\"])\n",
    "        lyrics_summaries[lyric[\"name\"].lower()] = {\n",
    "            \"summary\": summary,\n",
    "            \"text\": lyric[\"text\"]}\n",
    "        with open(\"/home/jupyter/self_learning/Langchain/code/fairytaleDJ/data/lyrics_with_summary.json\",\"w\") as f:\n",
    "            json.dump(lyrics_summaries, f)\n",
    "\n",
    "pprint(lyrics_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b61d4b-1901-4aca-ae69-05443a0691e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
