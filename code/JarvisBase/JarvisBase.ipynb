{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "06835027-491d-4731-bbea-4a841eaea39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import re\n",
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "import openai\n",
    "import streamlit as st\n",
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from streamlit_chat import message\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "80cb2123-bee1-43c1-893b-de70d927dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b6078bed-50b5-44d4-a7ee-f6b347d879b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bb876476-f223-4cdd-b5e4-be56944fb921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"\"\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"]= \"\"\n",
    "os.environ[\"GOOGLE_CSE_ID\"]= \"\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]= \"\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"\"\n",
    "os.environ[\"WOLFRAM_ALPHA_APPID\"] = \"\"\n",
    "os.environ[\"SERPAPI_API_KEY\"]= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "19da4f63-5983-46a8-81ff-692a6cf567cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ada =  OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "chromadb_collection_name= \"ada_hf_hub_doc_collection\"\n",
    "scraped_content_save_path= \"/home/jupyter/self_learning/Langchain/code/JarvisBase/data/hf_hub_doc_crawled/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3e4e062f-e177-4059-b176-055e1071a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documentation_urls():\n",
    "    # List of relative URLs for Hugging Face documentation pages, commented a lot of these because it would take too long to scrape all of them\n",
    "    return [\n",
    "    '/docs/huggingface_hub/guides/overview',\n",
    "    '/docs/huggingface_hub/guides/download',\n",
    "    '/docs/huggingface_hub/guides/upload',\n",
    "    '/docs/huggingface_hub/guides/hf_file_system',\n",
    "    '/docs/huggingface_hub/guides/repository',\n",
    "    '/docs/huggingface_hub/guides/search',\n",
    "    '/docs/huggingface_hub/guides/inference',\n",
    "    '/docs/huggingface_hub/guides/community',\n",
    "    '/docs/huggingface_hub/guides/manage-cache',\n",
    "    '/docs/huggingface_hub/guides/model-cards',\n",
    "    '/docs/huggingface_hub/guides/manage-spaces',\n",
    "    '/docs/huggingface_hub/guides/integrations',\n",
    "    '/docs/huggingface_hub/guides/webhooks_server', \n",
    "    #Add the rest of the URLs here\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dceadc49-8c12-4a1b-90e9-98215bf2256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_full_url(base_url, relative_url):\n",
    "    # Construct the full URL by appending the relative URL to the base URL\n",
    "    return base_url + relative_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "516d08aa-b9f2-43e0-ba96-897c3468dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_content(url):\n",
    "    # Send a GET request to the URL and parse the HTML response using BeautifulSoup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Extract the desired content from the page (in this case, the body text)\n",
    "    text=soup.body.text.strip()\n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', text)\n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5373e694-51f5-48f9-a4a3-964b83b46ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_content(base_url, relative_url, filepath):\n",
    "    # Loop through the list of URLs, scrape content, and add it to the content list\n",
    "#     content = []\n",
    "#     for relative_url in relative_urls:\n",
    "    full_url = construct_full_url(base_url, relative_url)\n",
    "    scraped_content = scrape_page_content(full_url)\n",
    "    scraped_content= scraped_content.rstrip('\\n')\n",
    "    filename= filepath+ relative_url.split(\"/\")[-1] + \".txt\"\n",
    "    # Write the scraped content to a file\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"%s\\n\" % scraped_content)\n",
    "    \n",
    "    return scraped_content, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "53fca63e-86f0-4283-a9c7-fc0ac4dd2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load documents from a file\n",
    "def load_docs(filename):\n",
    "    # Create an empty list to hold the documents\n",
    "    docs = []\n",
    "    try:\n",
    "        # Load the file using the TextLoader class and UTF-8 encoding\n",
    "        loader = TextLoader(filename, encoding='utf-8')\n",
    "        # Split the loaded file into separate documents and add them to the list of documents\n",
    "        docs.extend(loader.load_and_split())\n",
    "    except Exception as e:\n",
    "        # If an error occurs during loading, ignore it and return an empty list of documents\n",
    "        pass\n",
    "    # Return the list of documents\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "50569829-b77d-4cad-b58b-e8015868a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20, length_function=len)\n",
    "    return text_splitter.split_documents(docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eb4b5b44-5325-4b43-ad4f-a837ed5a151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors_into_db(documents):\n",
    "    client = chromadb.Client()\n",
    "    db = Chroma.from_documents(documents, embed_ada, client=client, collection_name=chromadb_collection_name)\n",
    "    print(\"There are\", db._collection.count(), \"in the collection\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "148eb7ee-d89e-4dea-85cc-7fedae9075f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/self_learning/Langchain/code/JarvisBase/data/hf_hub_doc_crawled/overview.txt\n",
      "There are 6 in the collection\n",
      "/home/jupyter/self_learning/Langchain/code/JarvisBase/data/hf_hub_doc_crawled/download.txt\n",
      "There are 32 in the collection\n",
      "/home/jupyter/self_learning/Langchain/code/JarvisBase/data/hf_hub_doc_crawled/upload.txt\n",
      "There are 97 in the collection\n",
      "/home/jupyter/self_learning/Langchain/code/JarvisBase/data/hf_hub_doc_crawled/hf_file_system.txt\n",
      "There are 110 in the collection\n",
      "/home/jupyter/self_learning/Langchain/code/JarvisBase/data/hf_hub_doc_crawled/repository.txt\n",
      "There are 136 in the collection\n",
      "/home/jupyter/self_learning/Langchain/code/JarvisBase/data/hf_hub_doc_crawled/search.txt\n",
      "There are 144 in the collection\n",
      "/home/jupyter/self_learning/Langchain/code/JarvisBase/data/hf_hub_doc_crawled/inference.txt\n",
      "There are 178 in the collection\n",
      "/home/jupyter/self_learning/Langchain/code/JarvisBase/data/hf_hub_doc_crawled/community.txt\n",
      "There are 194 in the collection\n",
      "/home/jupyter/self_learning/Langchain/code/JarvisBase/data/hf_hub_doc_crawled/manage-cache.txt\n",
      "There are 253 in the collection\n",
      "/home/jupyter/self_learning/Langchain/code/JarvisBase/data/hf_hub_doc_crawled/model-cards.txt\n",
      "There are 278 in the collection\n",
      "/home/jupyter/self_learning/Langchain/code/JarvisBase/data/hf_hub_doc_crawled/manage-spaces.txt\n",
      "There are 309 in the collection\n",
      "/home/jupyter/self_learning/Langchain/code/JarvisBase/data/hf_hub_doc_crawled/integrations.txt\n",
      "There are 342 in the collection\n",
      "/home/jupyter/self_learning/Langchain/code/JarvisBase/data/hf_hub_doc_crawled/webhooks_server.txt\n",
      "There are 366 in the collection\n"
     ]
    }
   ],
   "source": [
    "# Define the main function\n",
    "client = chromadb.Client()\n",
    "client.delete_collection(name=chromadb_collection_name)\n",
    "def main():\n",
    "    base_url = 'https://huggingface.co'\n",
    "    # Set the root directory where the content file will be saved\n",
    "    root_dir ='./'\n",
    "    relative_urls = get_documentation_urls()\n",
    "    # Scrape all the content from the relative urls and save it to the content file\n",
    "    for url in relative_urls:\n",
    "        content,filename = scrape_all_content(base_url, url, scraped_content_save_path)\n",
    "        # Load the content from the file\n",
    "        docs = load_docs(filename)\n",
    "        # Split the content into individual documents\n",
    "        docs = split_docs(docs)\n",
    "        # Create a DeepLake database with the given dataset path and embedding function\n",
    "        ada_hf_hub_doc_db = load_vectors_into_db(docs)\n",
    "\n",
    "# Call the main function if this script is being run as the main program\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "23ed6831-2bb8-4b3e-b8d5-01bd0a2fad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = chromadb.Client()\n",
    "# collection = client.get_collection(name=chromadb_collection_name, embedding_function=embed_ada)\n",
    "# print(\"There are\", collection.count(), \"in the collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1ce0699a-b5ab-405b-a79a-209c9f5a4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.get(include=['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f3dc13c9-d7aa-42a5-a3bc-f71412e58124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings and DeepLake database\n",
    "def load_embeddings_and_database(collection_name):\n",
    "    embed_ada =  OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    client = chromadb.Client()\n",
    "    db = Chroma(client = client, collection_name = collection_name, embedding_function=embed_ada)\n",
    "#     db = client.get_collection(name=collection_name, embedding_function=embed_ada)\n",
    "    print(\"There are\", db._collection.count(), \"in the collection\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "68a246cd-399a-44ef-9cfc-6bd569a73db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user input from Streamlit text input field\n",
    "def get_user_input():\n",
    "    return st.text_input(\"\", value=st.session_state.get(\"input\", \"Hello, how are you?\"), key=\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d76ed820-340b-473c-9b77-2b401e86ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the database for a response based on the user's query\n",
    "def search_db(user_input, db):\n",
    "#     print(\"Question:\", user_input)\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 10})\n",
    "    model = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "    qa = RetrievalQAWithSourcesChain.from_llm(model, retriever=retriever)\n",
    "    return qa({'question': user_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "42224f6b-b4ce-4bff-bc31-ac7977934310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display conversation history using Streamlit messages\n",
    "def display_conversation(history):\n",
    "    for i in range(len(history[\"generated\"])):\n",
    "        message(history[\"past\"][i], is_user=True, key=str(i) + \"_user\")\n",
    "        message(history[\"generated\"][i],key=str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ea2297f5-4f57-42d0-9fd8-12098e757964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 366 in the collection\n",
      "{'question': 'Write a code to Load a Model Card from the Hub?', 'answer': \"To load a Model Card from the Hub, you can use the following code:\\n\\n```python\\nfrom huggingface_hub import ModelCard\\n\\nmodel_card = ModelCard.load_model_card_from_hub(repo_id='username/repository_name')\\n```\\n\", 'sources': '/home/jupyter/self_learning/Langchain/code/JarvisBase/data/hf_hub_doc_crawled/model-cards.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Main function to run the app\n",
    "def main():\n",
    "    # Load embeddings and the DeepLake database\n",
    "    db = load_embeddings_and_database(chromadb_collection_name)\n",
    "    user_input= \"Write a code to Load a Model Card from the Hub?\"\n",
    "    output = search_db(user_input, db)\n",
    "    print(output)\n",
    "#     print(output['sources'])\n",
    "#     response = str(output[\"result\"])\n",
    "#     print(response)\n",
    "# Run the main function when the script is executed\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ecda4be-9652-49b7-8c9b-064077dbe93c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 18:41:32.391 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 in the collection\n",
      "Hello, how are you?\n",
      "[Document(page_content='Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up Hub Python Library documentation How-to guides Hub Python Library 🏡 View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerAutoTrainCompetitionsDatasetsDatasets-serverDiffusersEvaluateGradioHubHub Python LibraryHuggingface.jsInference API (serverless)Inference Endpoints (dedicated)OptimumPEFTSafetensorsTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jstimm Search documentation mainv0.21.4v0.20.3v0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 CNDEENFRHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout', metadata={'source': '/home/jupyter/self_learning/Langchain/code/JarvisBase/data/content.txt'}), Document(page_content=\"folder? How to make changes to an existing repository on the Hub? Search How to efficiently search through the 200k+ public models, datasets and spaces? HfFileSystem How to interact with the Hub through a convenient interface that mimics Python's file interface? Inference How to make predictions using the accelerated Inference API? Community Tab How to interact with the Community tab (Discussions and Pull Requests)? Collections How to programmatically build collections? Cache How does the cache-system work? How to benefit from it? Model Cards How to create and share Model Cards? Manage your Space How to manage your Space hardware and configuration? Integrate a library What does it mean to integrate a library with the Hub? And how to do it? Webhooks server How to create a server to receive Webhooks and deploy it as a Space? ←Installation Download files→ How-to guides\", metadata={'source': '/home/jupyter/self_learning/Langchain/code/JarvisBase/data/content.txt'})]\n"
     ]
    }
   ],
   "source": [
    "# # Main function to run the app\n",
    "# def main():\n",
    "#     # Initialize Streamlit app with a title\n",
    "#     st.write(\"# JarvisBase 🧙\")\n",
    "   \n",
    "#     # Load embeddings and the DeepLake database\n",
    "#     db = load_embeddings_and_database(chromadb_collection_name)\n",
    "\n",
    "#     # Record and transcribe audio\n",
    "# #     transcription = record_and_transcribe_audio()\n",
    "\n",
    "#     # Get user input from text input or audio transcription\n",
    "#     user_input = get_user_input()\n",
    "\n",
    "#     # Initialize session state for generated responses and past messages\n",
    "#     if \"generated\" not in st.session_state:\n",
    "#         st.session_state[\"generated\"] = [\"I am ready to help you\"]\n",
    "#     if \"past\" not in st.session_state:\n",
    "#         st.session_state[\"past\"] = [\"Hey there!\"]\n",
    "        \n",
    "#     # Search the database for a response based on user input and update session state\n",
    "#     if user_input:\n",
    "#         output = search_db(user_input, db)\n",
    "#         print(output['source_documents'])\n",
    "# #         st.session_state.past.append(user_input)\n",
    "#         response = str(output[\"result\"])\n",
    "# #         st.session_state.generated.append(response)\n",
    "\n",
    "#     # Display conversation history using Streamlit messages\n",
    "# #     if st.session_state[\"generated\"]:\n",
    "# #         display_conversation(st.session_state)\n",
    "\n",
    "# # Run the main function when the script is executed\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
