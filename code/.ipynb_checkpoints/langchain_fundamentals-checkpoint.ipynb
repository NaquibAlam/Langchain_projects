{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74223921-1256-4dc5-918d-72a5bb4c6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import DeepLake\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "from langchain.llms import GPT4All\n",
    "# from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from tqdm import tqdm\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import validator, field_validator\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import cohere\n",
    "from langchain.embeddings import CohereEmbeddings\n",
    "from langchain_community.llms import Cohere\n",
    "from langchain_community.chat_models import ChatCohere\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cbc5cfc-9e8d-42c0-a870-d08c32977960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b53e44e6-e9e5-48c5-9a2a-f03f59a79e89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ef1f8c3-a652-421e-bde7-985c224bdd88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"\"\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"]= \"\"\n",
    "os.environ[\"GOOGLE_CSE_ID\"]= \"\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]= \"\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"\"\n",
    "os.environ[\"WOLFRAM_ALPHA_APPID\"] = \"\"\n",
    "os.environ[\"SERPAPI_API_KEY\"]= \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994f4f9d-264a-49d8-81cb-9fb2c2ade250",
   "metadata": {},
   "source": [
    "# What is Langchain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b72b8-533d-4652-b9c0-d8542dcbf1c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "![title](../data/llm_os_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792733cf-c519-4522-a79f-77e96eacf066",
   "metadata": {},
   "source": [
    "![title](../data/llm_os_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30094375-e46f-40e6-94aa-8f926ae91474",
   "metadata": {},
   "source": [
    "![title](../data/llm_os_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50246ef-c3ee-47eb-a7ef-d3b073d9b61c",
   "metadata": {},
   "source": [
    "__LangChain__ is an innovative framework which interacts with all the modules of LLM OS and tailored for developing applications that leverage the capabilities of language models. \\\n",
    " it's a full-fledged ecosystem comprising several integral parts. \n",
    " * __LangChain__: helps in development of the application\n",
    " * __LangSmith__: helps in inspecting, testing, and monitoring the chains, ensuring that the applications are constantly improving and ready for deployment.\n",
    " * __LangServe__: helps in deploying or serving the application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613515c-fdbd-45dc-a748-673b66238429",
   "metadata": {},
   "source": [
    "LangChain allows for the creation of language model applications through various modules. These modules are:\n",
    "\n",
    "* __Model I/O__: Facilitates interaction with various language models, handling their inputs and outputs efficiently.\n",
    "* __Retrieval__: Enables access to and interaction with application-specific data, crucial for dynamic data utilization.\n",
    "* __Chains__: Offers pre-defined, reusable compositions that serve as building blocks for application development.\n",
    "* __Memory__: Maintains application state across multiple chain executions, essential for context-aware interactions.\n",
    "* __Tools__: Tools help users to effortlessly tap into a diverse range of functionalities and information sources to tackle challenges and generate meaningful responses.\n",
    "* __Agents__: Empower applications to select appropriate tools based on high-level directives, enhancing decision-making capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a912d7-ae93-4a97-8b90-1a63c76c7f60",
   "metadata": {},
   "source": [
    "# 1. Model I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02138d6-87f9-427d-9f45-a0e7382b4306",
   "metadata": {},
   "source": [
    "In LangChain, the core element of any application revolves around the language model. This module provides the essential building blocks to interface effectively with any language model, ensuring seamless integration and communication.\\\n",
    "__Key Components of Model I/O__\n",
    "1. __LLMs and Chat Models (used interchangeably)__:\n",
    "    * __LLMs__:\n",
    "        * Definition: Pure text completion models.\n",
    "        * Input/Output: Take a text string as input and return a text string as output.\n",
    "    * __Chat Models__\n",
    "        * Definition: Models that use a language model as a base but differ in input and output formats.\n",
    "        * Input/Output: Accept a list of chat messages as input and return a Chat Message.\n",
    "2. __Prompts__: Templatize, dynamically select, and manage model inputs. Allows for the creation of flexible and context-specific prompts that guide the language model's responses.\n",
    "3. __Output Parsers__: Extract and format information from model outputs. Useful for converting the raw output of language models into structured data or specific formats needed by the application.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a4e04-1bbe-4f8c-a849-c925ddd2507e",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fafb46c-917e-444a-b1f4-683c6bb47dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_openai = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.0)\n",
    "chat_openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aaae6d-e7e8-40a0-ae57-fac3452f78b0",
   "metadata": {},
   "source": [
    "LangChain offers a uniform interface to interact with various LLMs like OpenAI, Cohere, Hugging Face, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73de3e3f-f3d7-4253-8252-cced3039cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "# llm_openai = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.0)\n",
    "llm_hf = HuggingFaceHub(repo_id='google/flan-t5-large', model_kwargs={'temperature':0.0})\n",
    "# llm = Cohere(model=\"command\")\n",
    "llm_cohere = Cohere()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be7b25b7-8bc5-4e56-b4d3-c3e20089bb91",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================OPENAI GPT3.5 model==========================\n",
      "\n",
      "\n",
      "Monday:\n",
      "Warm-up: 10 minutes of brisk walking or jogging\n",
      "Main workout:\n",
      "- 30 minutes of cycling on a hilly route\n",
      "- 3 sets of 10 push-ups\n",
      "- 3 sets of 10 squats\n",
      "- 3 sets of 10 lunges\n",
      "Cool down: 5 minutes of stretching\n",
      "\n",
      "Tuesday:\n",
      "Warm-up: 10 minutes of jumping jacks or jump rope\n",
      "Main workout:\n",
      "- 30 minutes of running on a trail or track\n",
      "- 3 sets of 10 burpees\n",
      "- 3 sets of 10 mountain climbers\n",
      "- 3 sets of 10 tricep dips using a park bench\n",
      "Cool down: 5 minutes of stretching\n",
      "\n",
      "Wednesday:\n",
      "Rest day or low-intensity activity such as yoga or walking\n",
      "\n",
      "Thursday:\n",
      "Warm-up: 10 minutes of high knees or butt kicks\n",
      "Main workout:\n",
      "- 30 minutes of swimming laps in a pool or open water\n",
      "- 3 sets of 10 pull-ups using a playground bar\n",
      "- 3 sets of 10 jump squats\n",
      "- 3 sets of 10 Russian twists\n",
      "Cool down: 5 minutes of stretching\n",
      "\n",
      "Friday:\n",
      "Warm-up: 10 minutes of jogging in place or jumping jacks\n",
      "Main workout:\n",
      "\n",
      "==================HUGGIGFACE flan-t5-large model==========================\n",
      "During the day, walk for at least 30 minutes a day.\n",
      "==================COHERE DEFAULT model==========================\n",
      " Here's a suggested workout routine for someone looking to improve their cardiovascular endurance and prefers outdoor activities:\n",
      "\n",
      "1. Run or jog for 30 minutes at a steady pace: Start with a slower pace to warm up and gradually increase the speed to a challenging, but sustainable level. Aim for a consistent pace throughout the 30 minutes. You can mix in sprints here and there to really get your blood pumping. \n",
      "\n",
      "2. Hiking: Look for some nearby trails with varying degrees of incline to not only give you a great view at the top but tremendous cardiovascular benefits as well. Try to hike for at least 45 minutes to an hour and aim to do this workout twice a week. \n",
      "\n",
      "3. Bike riding: Biking is a low-impact cardio activity that increases endurance and stamina. You can either ride a traditional bike or alternatively, take advantage of the outdoors with a mountain bike trail when you can engage hills and varying terrain. Aim for 30-45 minutes in your beginning phases and work your way up from there. \n",
      "\n",
      "4. Swimming: Swimming is an excellent low-impact exercise that works your entire body and cardiovascular system. If you're already somewhat proficient, try doing some laps at a moderate pace for 20-30 minutes. If you're still a beginner, try\n"
     ]
    }
   ],
   "source": [
    "query = \"Suggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities.\"\n",
    "print(\"==================OPENAI GPT3.5 model==========================\")\n",
    "print(llm_openai(query))\n",
    "print(\"==================HUGGIGFACE flan-t5-large model==========================\")\n",
    "print(llm_hf(query))\n",
    "print(\"==================COHERE DEFAULT model==========================\")\n",
    "print(llm_cohere(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c58087f3-67a5-4cd8-82df-bfb8f1301b44",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================OPENAI GPT3.5 model==========================\n",
      "\n",
      "\n",
      "EcoHydrate Co.\n",
      "==================HUGGIGFACE flan-t5-large model==========================\n",
      "reusable\n",
      "==================COHERE DEFAULT model==========================\n",
      " Here are a few ideas for names for a company that makes eco-friendly water bottles:\n",
      "\n",
      "1. Green Guardian: This name suggests that the company is protecting the environment by providing eco-friendly products.\n",
      "\n",
      "2. Aqua Verde: This name combines the Spanish words for \"water\" and \"green,\" conveying a clear message about the company's focus on environmentally friendly water bottles.\n",
      "\n",
      "3. EcoHydrate: A catchy name that combines \"eco\" for ecology and \"hydrate\" for the purpose of the product, this name is easy to remember and conveys the dual purpose of environmental friendliness and hydration.\n",
      "\n",
      "4. EarthtoPouch: This name is a play on the word \"earth\" and \"pouch,\" suggesting an earth-friendly water bottle product. \n",
      "\n",
      "5. Blue Planet: This name is a direct reference to the popular idiom \"blue planet\" which symbolizes the uniqueness and fragility of our planet Earth, reminding customers of the company's eco-driven mission. \n",
      "\n",
      "6. Rewave: This name has a modern connotation and sounds like \"revive,\" conveying the company's mission to refresh and revive the planet using sustainable and eco-friendly means. \n",
      "\n",
      "These are just a few ideas, take any one\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a good name for a company that makes eco-friendly water bottles.\"\n",
    "print(\"==================OPENAI GPT3.5 model==========================\")\n",
    "print(llm_openai(query))\n",
    "print(\"==================HUGGIGFACE flan-t5-large model==========================\")\n",
    "print(llm_hf(query))\n",
    "print(\"==================COHERE DEFAULT model==========================\")\n",
    "print(llm_cohere(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46777827-f890-4aeb-8a05-5e1726eb31c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"List of seven wonders of world\"\n",
    "# print(\"==================OPENAI GPT3.5 model==========================\")\n",
    "# print(llm_openai(query))\n",
    "# print(\"==================HUGGIGFACE flan-t5-large model==========================\")\n",
    "# print(llm_hf(query))\n",
    "# print(\"==================COHERE DEFAULT model==========================\")\n",
    "# print(llm_cohere(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e7437-b75b-49cf-b596-c94402ae6e84",
   "metadata": {},
   "source": [
    "## Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5850612c-b7f5-4507-9e71-8fc06371fc0c",
   "metadata": {},
   "source": [
    "LangChain's integration with chat models, a specialized variation of language models, is essential for creating interactive chat applications. While they utilize language models internally, chat models present a distinct interface centered around chat messages as inputs and outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e93d4448-030b-4d66-8a45-3797c7916b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am associated with Nike.\n"
     ]
    }
   ],
   "source": [
    "# chat_openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "# cohere_chat_model = ChatCohere()\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are Micheal Jordan.\"),\n",
    "    HumanMessage(content=\"Which shoe manufacturer are you associated with?\"),\n",
    "]\n",
    "response = chat_openai(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae0eaf35-f9ac-4d98-a0d8-d7f67264e74f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Schindler's List\" is a 1993 American historical drama film directed by Steven Spielberg. The film is based on the novel \"Schindler's Ark\" by Thomas Keneally and tells the true story of Oskar Schindler, a German businessman who saved the lives of more than a thousand Polish-Jewish refugees during the Holocaust by employing them in his factories.\n",
      "\n",
      "The film stars Liam Neeson as Oskar Schindler, Ben Kingsley as Itzhak Stern, and Ralph Fiennes as Amon Goeth. \"Schindler's List\" received critical acclaim and won seven Academy Awards, including Best Picture, Best Director, and Best Original Score.\n",
      "\n",
      "The movie is known for its powerful portrayal of the horrors of the Holocaust and the heroism of those who risked their lives to save others. It is considered one of the greatest films ever made and is often included in lists of the best movies of all time.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are an assistant that helps users find information about movies.\"),\n",
    "    HumanMessage(content=\"Find information about the movie Schindler's List\"),\n",
    "]\n",
    "response = chat_openai(messages)\n",
    "print(response)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ba3f752-c77a-4e0f-8486-0328d1d11e0e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you enjoyed \"Schindler's List\" and are looking for similar movies that explore themes of the Holocaust, World War II, and human resilience, here are some recommendations:\n",
      "\n",
      "1. \"The Pianist\" (2002) - Directed by Roman Polanski, this film tells the true story of Wladyslaw Szpilman, a Polish-Jewish pianist who survived the Holocaust in Warsaw.\n",
      "\n",
      "2. \"Life is Beautiful\" (1997) - An Italian film directed by and starring Roberto Benigni, which tells the story of a Jewish father who uses humor and imagination to protect his son in a concentration camp.\n",
      "\n",
      "3. \"The Boy in the Striped Pyjamas\" (2008) - Based on the novel by John Boyne, this film follows the unlikely friendship between a young German boy and a Jewish boy in a concentration camp.\n",
      "\n",
      "4. \"Sophie's Choice\" (1982) - Starring Meryl Streep and Kevin Kline, this film explores the devastating choices faced by a Polish woman during the Holocaust.\n",
      "\n",
      "5. \"The Diary of Anne Frank\" (1959) - Based on the real-life diary of Anne Frank, this film depicts the experiences of a Jewish family hiding from the Nazis in Amsterdam.\n",
      "\n",
      "These movies offer powerful and moving portrayals of the impact of the Holocaust on individuals and communities, and they showcase the resilience and courage of those who lived through this dark period in history.\n"
     ]
    }
   ],
   "source": [
    "messages.append(response)\n",
    "prompt= HumanMessage(content= \"Can you also recommend movies similar to this?\")\n",
    "messages.append(prompt)\n",
    "response= chat_openai(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaa89d6-6014-4ba3-af24-c27744169982",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebb40f4e-2e5b-4f71-b133-3ef179747957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about robots.\n",
      "\n",
      "\n",
      "Why did the robot go on a diet?\n",
      "\n",
      "Because he wanted to reduce his \"byte\" size!\n"
     ]
    }
   ],
   "source": [
    "# Simple prompt with placeholders\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "\n",
    "# Filling placeholders to create a prompt\n",
    "filled_prompt = prompt_template.format(adjective=\"funny\", content=\"robots\")\n",
    "print(filled_prompt)\n",
    "print(llm_openai(filled_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "369dfc5a-4502-458b-8fb8-b75315e4af98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital city of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"What is the capital city of France?\"\n",
    "filled_prompt = prompt.format(question=question)\n",
    "print(llm_openai(filled_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d63ac7f-ea9a-4a0b-98d9-ed4251008aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'), HumanMessage(content='Hello, how are you doing?'), AIMessage(content=\"I'm doing well, thanks!\"), HumanMessage(content='What is your name?')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='My name is Bob. How can I assist you today?')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a chat prompt with various roles\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Formatting the chat prompt\n",
    "formatted_messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")\n",
    "print(formatted_messages)\n",
    "chat_openai(formatted_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d75eb63d-efcb-4f10-9cc1-9cabaa6c2ec4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ladakh is a region in the northern part of India, located in the state of Jammu and Kashmir. It is known for its stunning landscapes, high mountain ranges, and unique culture. Here are some key points about Ladakh:\n",
      "\n",
      "1. Geography: Ladakh is situated between the Kunlun mountain range in the north and the main Great Himalayas to the south. It is a high-altitude desert with barren mountains, deep valleys, and crystal-clear lakes.\n",
      "\n",
      "2. Culture: Ladakh has a rich cultural heritage influenced by Tibetan Buddhism. The region is dotted with monasteries, stupas, and prayer flags. The people of Ladakh are known for their warmth and hospitality.\n",
      "\n",
      "3. Tourism: Ladakh is a popular tourist destination, especially for adventure enthusiasts. Visitors can enjoy activities such as trekking, mountaineering, river rafting, and camping. The famous Pangong Lake and Nubra Valley are must-visit attractions in Ladakh.\n",
      "\n",
      "4. Climate: Ladakh experiences extreme weather conditions with cold winters and mild summers. Due to its high altitude, visitors are advised to acclimatize properly to prevent altitude sickness.\n",
      "\n",
      "5. Leh: Leh is the largest town in Ladakh and serves as the main hub for tourists. It has a bustling market, ancient monasteries like Thiksey and Hemis, and the historic Leh Palace.\n",
      "\n",
      "Overall, Ladakh is a unique destination that offers a blend of natural beauty, cultural richness, and adventure opportunities.\n"
     ]
    }
   ],
   "source": [
    "template = \"You are an assistant that helps users find information about places.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"Find information about {place}.\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "formatted_messages= chat_prompt.format_messages(place=\"Ladakh\")\n",
    "response = chat_openai(formatted_messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439f635-68af-4aa2-83b7-0340ae3d8833",
   "metadata": {},
   "source": [
    "## Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f957dd49-7e41-4958-ba08-d816b11e46c4",
   "metadata": {},
   "source": [
    "While the language models can only generate textual outputs, a predictable data structure is always preferred in a production environment. \n",
    "\n",
    "The Output Parsers help create a data structure to define the expectations from the output precisely. We can ask for a list of words in case of a Thesaurus application or a combination of different variables like a word and the explanation of why it fits. The parser can extract the expected information for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c04cb0-a885-403c-9919-2437654cee8b",
   "metadata": {},
   "source": [
    "### PydanticOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4424a052-4c2f-4418-8709-7d5f4736fec2",
   "metadata": {},
   "source": [
    "We always import and follow the necessary libraries by creating the Suggestions schema class. There are two essential parts to this class:\n",
    "1. __Expected Outputs__: Each output is defined by declaring a variable with desired type, like a list of strings (: List[str]) in the sample code, or it could be a single string (: str) if you are expecting just one word/sentence as the response. Also, It is required to write a simple explanation using the Field function’s description attribute to help the model during inference. (We will see an example of having multiple outputs later in the lesson)\n",
    "2. __field_validator__: It is possible to declare functions to validate the formatting. We ensure that the first character is not a number in the sample code. The function’s name is unimportant, but the @field_validator decorator must receive the same name as the variable you want to approve. (like @validator(’words’)) It is worth noting that the field variable inside the validator function will be a list if you specify it as one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "896e1d3b-5b7c-4174-ae3e-b474f1278d50",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(place_name=['Leh', 'Manali', 'Darjeeling', 'Shimla', 'Gulmarg'], reasons=['Leh is known for its breathtaking views of the Himalayas and offers a unique cultural experience with its Tibetan Buddhist influence.', 'Manali is a popular destination for adventure seekers with its many trekking and skiing opportunities.', 'Darjeeling is a charming hill station with stunning views of the Kanchenjunga mountain range and is famous for its tea plantations.', 'Shimla is a picturesque hill station with colonial architecture and offers a peaceful retreat in the mountains.', 'Gulmarg is a paradise for skiing enthusiasts and offers stunning views of the snow-capped peaks of the Pir Panjal range.'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define your desired data structure using Pydantic\n",
    "class Suggestions(BaseModel):\n",
    "    place_name: List[str] = Field(description=\"list of places to travel based on preference\")\n",
    "    reasons: List[str] = Field(description=\"the reasoning for why this place fits the preference\")\n",
    "    \n",
    "    @field_validator('place_name')\n",
    "    def not_start_with_number(cls, v):\n",
    "        for item in v:\n",
    "            if item[0].isnumeric():\n",
    "                raise ValueError(\"The name of the place can not start with numbers!\")\n",
    "        return v\n",
    "    \n",
    "    @field_validator('reasons')\n",
    "    def not_more_than_3_sentences(cls, v):\n",
    "        for idx, item in enumerate( v ):\n",
    "            sentences= item.split(\".\")\n",
    "            if len(sentences) >3:\n",
    "                v[idx]= sentences[:3]\n",
    "        return v\n",
    "    \n",
    "\n",
    "# Set up a PydanticOutputParser\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=Suggestions)\n",
    "\n",
    "template = \"\"\"\n",
    "You are an assistant that helps users to plan trips.\n",
    "Suggest a list of places to visit in {country} for someone who loves {preference} with proper reasoning .\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"country\", \"preference\"],\n",
    "    partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "model_input = prompt.format(country=\"India\", preference=\"mountains\")\n",
    "output = llm_openai(model_input)\n",
    "pydantic_parser.parse(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95a063f7-e13f-41bb-a164-021ef5b889fa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an assistant that helps users to plan trips.\n",
      "Suggest a list of places to visit in India for someone who loves mountains with proper reasoning .\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"place_name\": {\"description\": \"list of places to travel based on preference\", \"items\": {\"type\": \"string\"}, \"title\": \"Place Name\", \"type\": \"array\"}, \"reasons\": {\"description\": \"the reasoning for why this place fits the preference\", \"items\": {\"type\": \"string\"}, \"title\": \"Reasons\", \"type\": \"array\"}}, \"required\": [\"place_name\", \"reasons\"]}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(model_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb805b5-37c7-4caf-ac4d-3e247146847d",
   "metadata": {},
   "source": [
    "### JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9f52341c-53fd-4015-96a4-01feb54a310e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'places': [{'name': 'Leh-Ladakh',\n",
       "   'reason': 'Known for its breathtaking landscapes, high altitude passes, and adventure activities like trekking and mountaineering.'},\n",
       "  {'name': 'Manali',\n",
       "   'reason': 'A popular hill station with stunning views of the Himalayas, perfect for skiing, paragliding, and other outdoor activities.'},\n",
       "  {'name': 'Darjeeling',\n",
       "   'reason': 'Famous for its tea plantations, scenic views of the Kanchenjunga mountain, and the Darjeeling Himalayan Railway.'},\n",
       "  {'name': 'Shimla',\n",
       "   'reason': 'A charming hill station with colonial architecture, surrounded by snow-capped mountains and offering activities like skiing and trekking.'},\n",
       "  {'name': 'McLeod Ganj',\n",
       "   'reason': 'Located in the foothills of the Dhauladhar range, this town is known for its Tibetan culture, monasteries, and trekking opportunities.'}]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Initialize the JSON parser\n",
    "json_parser = JsonOutputParser()\n",
    "\n",
    "template = \"\"\"\n",
    "You are an assistant that helps users to plan trips.\n",
    "Suggest a list of places to visit in {country} for someone who loves {preference} with proper reasoning.\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"country\", \"preference\"],\n",
    "    partial_variables={\"format_instructions\": json_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "model_input = prompt.format(country=\"India\", preference=\"mountains\")\n",
    "output = llm_openai(model_input)\n",
    "json_parser.parse(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa837425-4f1d-4a3f-858d-c0f9c460bf6e",
   "metadata": {},
   "source": [
    "### CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ab76e87f-9987-42dc-b834-df6c3288dc84",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Manali',\n",
       " 'Shimla',\n",
       " 'Leh',\n",
       " 'Darjeeling',\n",
       " 'Sikkim',\n",
       " 'McLeod Ganj',\n",
       " 'Auli',\n",
       " 'Nainital',\n",
       " 'Mussoorie',\n",
       " 'Gangtok']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize the CSV parser\n",
    "csv_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "template = \"\"\"\n",
    "You are an assistant that helps users to plan trips.\n",
    "Suggest a list of places to visit in {country} for someone who loves {preference}.\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"country\", \"preference\"],\n",
    "    partial_variables={\"format_instructions\": csv_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "model_input = prompt.format(country=\"India\", preference=\"mountains\")\n",
    "output = llm_openai(model_input)\n",
    "csv_parser.parse(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94d66ea-670b-4e76-a20f-eef49a1f64c8",
   "metadata": {},
   "source": [
    "# 2. Retrieval (RAG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258f445b-dd68-4849-b048-695c80be8b63",
   "metadata": {},
   "source": [
    "Retrieval in LangChain plays a crucial role in applications that require user-specific data, not included in the model's training set. This process, known as Retrieval Augmented Generation (RAG), involves fetching external data and integrating it into the language model's generation process. \n",
    "\n",
    "LangChain provides a comprehensive suite of tools and functionalities to facilitate this process, catering to both simple and complex applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c072c99-63b4-439d-ac25-2d4c77b25ea9",
   "metadata": {},
   "source": [
    "## Document Loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552e29c-f32c-4470-8506-29cc21dc5365",
   "metadata": {},
   "source": [
    "Document loaders in LangChain enable the extraction of data from various sources. All these loaders ingest data into __Document__ classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f32ee6e-1938-4988-b1b5-719795879b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "path= \"/home/jupyter/self_learning/Langchain/data/\"\n",
    "############ Text File Loader #############\n",
    "text_loader = TextLoader(path+\"my_file.txt\")\n",
    "text_doc = text_loader.load()\n",
    "\n",
    "############ CSV File Loader #############\n",
    "csv_loader = CSVLoader(path+'mo_val_cg_100k_mexico_items_10k_sample_3k.csv')\n",
    "csv_doc = csv_loader.load()\n",
    "\n",
    "############ PDF File Loader #############\n",
    "pdf_loader = PyPDFLoader(path+\"Resume_Naquib_Alam.pdf\")\n",
    "pdf_doc = pdf_loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a5cccb8a-174a-488d-ab47-b79fc1bf2f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_doc[0].page_content.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50316d89-a224-42ff-9790-1adf2e83e4eb",
   "metadata": {},
   "source": [
    "Apart from these standard __Document Loaders__ LangChain offers a wide variety of custom loaders to directly load data from your apps (such as __Slack, Figma, Notion, Confluence, Google Drive, HTML dcouments__, etc.) and databases and use them in LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00653f-e58b-4e65-9175-cb3e7a9cbe0a",
   "metadata": {},
   "source": [
    "## Document Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8946f6-057c-47c1-9db0-c370e086e962",
   "metadata": {},
   "source": [
    "Document transformers in LangChain are essential tools designed to manipulate documents, which we created in our previous subsection.\n",
    "\n",
    "They are used for tasks such as splitting long documents into smaller chunks, combining, and filtering, which are crucial for adapting documents to a model's context window or meeting specific application needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b1fe3-86b2-4cb9-9239-62d09acde5c7",
   "metadata": {},
   "source": [
    "### Character Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "868e9429-3855-4a1a-a14e-5f3d1ed527f5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 373, which is longer than the specified 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 2 documents\n",
      "[Document(page_content='Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\\nGoogle is offering developers access to one of its most advanced AI language models: PaLM.\\nThe search giant is launching an API for PaLM alongside a number of AI enterprise tools\\nit says will help businesses “generate text, images, code, videos, audio, and more from\\nsimple natural language prompts.”', metadata={'source': '/home/jupyter/self_learning/Langchain/data/my_file.txt'}), Document(page_content='PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\\nMeta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\\nPaLM is a flexible system that can potentially carry out all sorts of text generation and\\nediting tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\\nexample, or you could use it for tasks like summarizing text or even writing code.\\n(It’s similar to features Google also announced today for its Workspace apps like Google\\nDocs and Gmail.)', metadata={'source': '/home/jupyter/self_learning/Langchain/data/my_file.txt'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=100, chunk_overlap=20, length_function=len)\n",
    "cts_docs = text_splitter.split_documents(text_doc)\n",
    "\n",
    "print (f\"You have {len(cts_docs)} documents\")\n",
    "print (cts_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e272ceb3-8884-457d-81d6-38271ea2ad40",
   "metadata": {},
   "source": [
    "### Recursive Character Text Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6669e-a4fc-453f-96fc-ba86e89ce33c",
   "metadata": {},
   "source": [
    "The Recursive Character Text Splitter is a text splitter designed to split the text into chunks based on a list of characters provided. It attempts to split text using the characters from a list in order until the resulting chunks are small enough. By default, the list of characters used for splitting is __[\"\\n\\n\", \"\\n\", \" \", \"]__, which tries to keep paragraphs, sentences, and words together as long as possible, as they are generally the most semantically related pieces of text. This means that the class first tries to split the text into two new-line characters. If the resulting chunks are still larger than the desired chunk size, it will then try to split the output by a single new-line character, followed by a space character, and so on, until the desired chunk size is achieved.\n",
    "\n",
    "To use the RecursiveCharacterTextSplitter, you can create an instance of it and provide the following parameters:\n",
    "* __chunk_size__ : The maximum size of the chunks, as measured by the length_function (default is 100).\n",
    "* __chunk_overlap__: The maximum overlap between chunks to maintain continuity between them (default is 20).\n",
    "* __length_function__: parameter is used to calculate the length of the chunks. By default, it is set to len, which counts the number of characters in a chunk. However, you can also pass a token counter or any other function that calculates the length of a chunk based on your specific requirements.\n",
    "\n",
    "Using a token counter instead of the default __len__ function can benefit specific scenarios, such as when working with language models with token limits. For example, OpenAI's GPT-3 has a token limit of 4096 tokens per request, so you might want to count tokens instead of characters to better manage and optimize your requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1b75680-86fb-4655-8514-1409edf4d305",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 12 documents\n",
      "[Document(page_content='Google opens up its AI language model PaLM to challenge OpenAI and GPT-3', metadata={'source': '/home/jupyter/self_learning/Langchain/data/my_file.txt'}), Document(page_content='Google is offering developers access to one of its most advanced AI language models: PaLM.', metadata={'source': '/home/jupyter/self_learning/Langchain/data/my_file.txt'}), Document(page_content='The search giant is launching an API for PaLM alongside a number of AI enterprise tools', metadata={'source': '/home/jupyter/self_learning/Langchain/data/my_file.txt'}), Document(page_content='it says will help businesses “generate text, images, code, videos, audio, and more from', metadata={'source': '/home/jupyter/self_learning/Langchain/data/my_file.txt'}), Document(page_content='simple natural language prompts.”', metadata={'source': '/home/jupyter/self_learning/Langchain/data/my_file.txt'}), Document(page_content='PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or', metadata={'source': '/home/jupyter/self_learning/Langchain/data/my_file.txt'}), Document(page_content='Meta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,', metadata={'source': '/home/jupyter/self_learning/Langchain/data/my_file.txt'}), Document(page_content='PaLM is a flexible system that can potentially carry out all sorts of text generation and', metadata={'source': '/home/jupyter/self_learning/Langchain/data/my_file.txt'}), Document(page_content='editing tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for', metadata={'source': '/home/jupyter/self_learning/Langchain/data/my_file.txt'}), Document(page_content='example, or you could use it for tasks like summarizing text or even writing code.', metadata={'source': '/home/jupyter/self_learning/Langchain/data/my_file.txt'}), Document(page_content='(It’s similar to features Google also announced today for its Workspace apps like Google', metadata={'source': '/home/jupyter/self_learning/Langchain/data/my_file.txt'}), Document(page_content='Docs and Gmail.)', metadata={'source': '/home/jupyter/self_learning/Langchain/data/my_file.txt'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20, length_function=len)\n",
    "rcts_docs = text_splitter.split_documents(text_doc)\n",
    "print (f\"You have {len(rcts_docs)} documents\")\n",
    "print (rcts_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d63cbdf-225a-4342-a93f-75d8dec26e34",
   "metadata": {},
   "source": [
    "### TokenTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0595d40f-6f20-4a80-92a2-c92ea005d4c5",
   "metadata": {},
   "source": [
    "The main advantage of using TokenTextSplitter over other text splitters, like CharacterTextSplitter, is that it respects the token boundaries, ensuring that the chunks do not split tokens in the middle. This can be particularly helpful in maintaining the semantic integrity of the text when working with language models and embeddings.\n",
    "\n",
    "This type of splitter breaks down raw text strings into smaller pieces by initially converting the text into BPE (Byte Pair Encoding) tokens, and subsequently dividing these tokens into chunks. It then reassembles the tokens within each chunk back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e88423a1-d2dc-4421-932f-89fcf368e325",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 3 documents\n",
      "[Document(page_content='Google opens up its AI language model PaLM to challenge OpenAI and GPT-3\\nGoogle is offering developers access to one of its most advanced AI language models: PaLM.\\nThe search giant is launching an API for PaLM alongside a number of AI enterprise tools\\nit says will help businesses “generate text, images, code, videos, audio, and more from\\nsimple natural language prompts.”\\n\\nPaLM is a large language model, or LLM,', metadata={'source': '/home/jupyter/self_learning/Langchain/data/my_file.txt'}), Document(page_content=' natural language prompts.”\\n\\nPaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or\\nMeta’s LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs,\\nPaLM is a flexible system that can potentially carry out all sorts of text generation and\\nediting tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for\\nexample', metadata={'source': '/home/jupyter/self_learning/Langchain/data/my_file.txt'}), Document(page_content=' You could train PaLM to be a conversational chatbot like ChatGPT, for\\nexample, or you could use it for tasks like summarizing text or even writing code.\\n(It’s similar to features Google also announced today for its Workspace apps like Google\\nDocs and Gmail.)\\n', metadata={'source': '/home/jupyter/self_learning/Langchain/data/my_file.txt'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Initialize the TokenTextSplitter with desired chunk size and overlap\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "\n",
    "# Split into smaller chunks\n",
    "tts_docs = text_splitter.split_documents(text_doc)\n",
    "print (f\"You have {len(tts_docs)} documents\")\n",
    "print (tts_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6529c942-956a-4843-b81a-83b5b6367395",
   "metadata": {},
   "source": [
    "The __chunk_size__ parameter sets the maximum number of BPE tokens in each chunk, while __chunk_overlap__ defines the number of overlapping tokens between adjacent chunks. By modifying these parameters, you can fine-tune the granularity of the text chunks.\n",
    "\n",
    "One potential drawback of using ___TokenTextSplitter___ is that it may require additional computation when converting text to BPE tokens and back. If you need a faster and simpler text-splitting method, you might consider using CharacterTextSplitter, which directly splits the text based on character count, offering a more straightforward approach to text segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e56c35e-92a0-4529-ad70-529d1b128867",
   "metadata": {},
   "source": [
    "## Text Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0841311c-21e3-4ba3-b230-9f5269f2b010",
   "metadata": {},
   "source": [
    "Text embedding models in LangChain provide a standardized interface for various embedding model providers like __OpenAI, Cohere, Hugging Face, etc.__ These models transform text into vector representations, enabling operations like semantic search through text similarity in vector space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33334308-af95-4923-8455-a48775349def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_similar_docs(model, query, docs, topk=5):\n",
    "    # Generate embeddings for the documents and query\n",
    "    document_embeddings = model.embed_documents(docs)\n",
    "    query_embedding = model.embed_query(query)\n",
    "    # Calculate similarity scores\n",
    "    similarity_scores = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "    # Find top-k similar document\n",
    "    sorted_similarity_scores_idx = np.argsort(-similarity_scores)[:topk]\n",
    "    topk_docs= [docs[i] for i in sorted_similarity_scores_idx]\n",
    "    return topk_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a846c0f-d967-4d54-ab7f-8e9b61fed86b",
   "metadata": {},
   "source": [
    "### OpenAI Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7d0e4bfd-5d38-4efc-96a2-376a8c459f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========OpenAI Embeddings===========\n",
      "Most similar document to the query 'A cat is sitting on a mat.':\n",
      "['The cat is on the mat.', 'There is a cat on the mat.']\n"
     ]
    }
   ],
   "source": [
    "## OpenAI Embedding\n",
    "# Define the documents\n",
    "print(\"=========OpenAI Embeddings===========\")\n",
    "documents = [\n",
    "    \"The cat is on the mat.\",\n",
    "    \"There is a cat on the mat.\",\n",
    "    \"The dog is in the yard.\",\n",
    "    \"There is a dog in the yard.\",\n",
    "]\n",
    "\n",
    "query = \"A cat is sitting on a mat.\"\n",
    "k= 2\n",
    "# Initialize the OpenAIEmbeddings instance\n",
    "openai_embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "topk_similar_docs = get_topk_similar_docs(openai_embed_model, query, documents, k)\n",
    "\n",
    "print(f\"top-{k} similar document to the query '{query}':\")\n",
    "print(topk_similar_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f586d5-1897-4248-b1a4-ee889da9ffc3",
   "metadata": {},
   "source": [
    "### HuggingFace Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ede3256a-c047-4d67-a253-1a3c72252b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========HuggingFace Embeddings===========\n",
      "top-2 similar document to the query 'A cat is sitting on a mat.':\n",
      "['The cat is on the mat.', 'There is a cat on the mat.']\n"
     ]
    }
   ],
   "source": [
    "## HuggingFace Embedding\n",
    "print(\"=========HuggingFace Embeddings===========\")\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cuda:0'}\n",
    "mpnet_embed_model = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "topk_similar_docs = get_topk_similar_docs(mpnet_embed_model, query, documents, k)\n",
    "\n",
    "print(f\"top-{k} similar document to the query '{query}':\")\n",
    "print(topk_similar_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c9145-7ef5-4a73-a7e4-e5a35306773f",
   "metadata": {},
   "source": [
    "### Cohere Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5d3ddf65-719c-419d-9a1c-6cc8b9a6b319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========Cohere Embeddings===========\n",
      "top-5 similar document to the query 'पेरिस बहुत खूबसूरत शहर है.':\n",
      "['पेरिस बहुत खूबसूरत शहर है.', 'Paris is a very beautiful city.', 'Paris est une très belle ville.', 'París es una ciudad muy hermosa.', 'कोहेरे से नमस्ते!']\n"
     ]
    }
   ],
   "source": [
    "## Cohere Embedding\n",
    "print(\"=========Cohere Embeddings===========\")\n",
    "\n",
    "# Define a list of texts\n",
    "documents= [\n",
    "    \"Hello from Cohere!\", \n",
    "    \"مرحبًا من كوهير!\", \n",
    "    \"Hallo von Cohere!\",  \n",
    "    \"Bonjour de Cohere!\", \n",
    "    \"¡Hola desde Cohere!\", \n",
    "    \"Olá do Cohere!\",  \n",
    "    \"Ciao da Cohere!\", \n",
    "    \"您好，来自 Cohere！\", \n",
    "    \"कोहेरे से नमस्ते!\",\n",
    "    \"Paris is a very beautiful city.\",\n",
    "    \"París es una ciudad muy hermosa.\",\n",
    "    \"Paris est une très belle ville.\",\n",
    "    \"पेरिस बहुत खूबसूरत शहर है.\"\n",
    "]\n",
    "\n",
    "query_lst = [\"Hello from Cohere!\", \"पेरिस बहुत खूबसूरत शहर है.\",  \"您好，来自 Cohere！\"]\n",
    "k=5 \n",
    "# Initialize the CohereEmbeddings object\n",
    "cohere_ml_embed_model = CohereEmbeddings(model=\"embed-multilingual-v2.0\")\n",
    "\n",
    "topk_similar_docs = get_topk_similar_docs(cohere_ml_embed_model, query_lst[1], documents, k)\n",
    "\n",
    "print(f\"top-{k} similar document to the query '{query_lst[1]}':\")\n",
    "print(topk_similar_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e94bf3-60cd-45f2-bf21-0e7f82876b5b",
   "metadata": {},
   "source": [
    "## Vector Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee7ba9-2a2b-4dfb-8f7d-221907d84ea8",
   "metadata": {},
   "source": [
    "Vector stores in LangChain support the efficient storage and searching of text embeddings. LangChain integrates with over 50 vector stores, providing a standardized interface for ease of use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "98cc55ca-86d4-46e9-a87c-fdac5e4e7897",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 in the collection\n",
      "top-5 similar document to the query 'पेरिस बहुत खूबसूरत शहर है.':\n",
      "[Document(page_content='पेरिस बहुत खूबसूरत शहर है.'), Document(page_content='Paris is a very beautiful city.'), Document(page_content='Paris est une très belle ville.'), Document(page_content='París es una ciudad muy hermosa.'), Document(page_content='कोहेरे से नमस्ते!')]\n"
     ]
    }
   ],
   "source": [
    "chromadb_client = chromadb.Client()\n",
    "db_cohere_ml_embed = Chroma.from_texts(\n",
    "    documents, cohere_ml_embed_model, client=chromadb_client, collection_name=\"tmp_collection\"\n",
    ")\n",
    "\n",
    "print(\"There are\", db_cohere_ml_embed._collection.count(), \"in the collection\")\n",
    "\n",
    "topk_similar_docs = db_cohere_ml_embed.similarity_search(query_lst[1], k=5) ##By default chromadb returns 4 documents\n",
    "\n",
    "print(f\"top-{k} similar document to the query '{query_lst[1]}':\")\n",
    "print(topk_similar_docs)\n",
    "\n",
    "# chromadb_client.delete_collection(name=\"tmp_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef9f111-757b-4ec2-93b2-c31a58bf8fdc",
   "metadata": {},
   "source": [
    "## Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e363db1-2b90-4212-820f-d7448429ea9d",
   "metadata": {},
   "source": [
    "Retrievers in LangChain are interfaces that return documents in response to an unstructured query. They are more general than vector stores, focusing on retrieval rather than storage. Although vector stores can be used as a retriever's backbone, there are other types of retrievers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8787a6b3-ba65-4ae8-be4f-bf5e7221ad48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-5 similar document to the query 'पेरिस बहुत खूबसूरत शहर है.':\n",
      "[Document(page_content='पेरिस बहुत खूबसूरत शहर है.'), Document(page_content='Paris is a very beautiful city.'), Document(page_content='Paris est une très belle ville.'), Document(page_content='París es una ciudad muy hermosa.'), Document(page_content='कोहेरे से नमस्ते!')]\n"
     ]
    }
   ],
   "source": [
    "retriever = db_cohere_ml_embed.as_retriever(search_kwargs={\"k\": 5})\n",
    "topk_retrieved_docs = retriever.get_relevant_documents(query_lst[1])\n",
    "print(f\"top-{k} similar document to the query '{query_lst[1]}':\")\n",
    "print(topk_retrieved_docs)\n",
    "\n",
    "chromadb_client.delete_collection(name=\"tmp_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6114c2ba-52c4-4205-95f9-10f562e32caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 124 documents\n",
      "There are 124 in the collection\n",
      "OpenAI\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10, chunk_overlap=0, length_function=len)\n",
    "rcts_docs = text_splitter.split_documents(text_doc)\n",
    "print (f\"You have {len(rcts_docs)} documents\")\n",
    "# print (rcts_docs)\n",
    "\n",
    "\n",
    "openai_embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "chromadb_client = chromadb.Client()\n",
    "db_openai_embed = Chroma.from_documents(\n",
    "    rcts_docs, openai_embed_model, client=chromadb_client, collection_name=\"tmp_collection\"\n",
    ")\n",
    "\n",
    "print(\"There are\", db_openai_embed._collection.count(), \"in the collection\")\n",
    "\n",
    "retriever = db_openai_embed.as_retriever(search_kwargs={\"k\": 5})\n",
    "query= \"How Google plans to challenge OpenAI?\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "print(retrieved_docs[0].page_content)\n",
    "\n",
    "chromadb_client.delete_collection(name=\"tmp_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0f3e6cce-00da-43eb-8a39-53f9a20b3a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain.retrievers.web_research import WebResearchRetriever\n",
    "\n",
    "# # Initialize components\n",
    "# chat_openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "# search = GoogleSearchAPIWrapper()\n",
    "# openai_embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "# vectorstore = Chroma(embedding_function= openai_embed_model)\n",
    "\n",
    "# # Instantiate WebResearchRetriever\n",
    "# web_research_retriever = WebResearchRetriever.from_llm(vectorstore=vectorstore, llm=chat_openai, search=search)\n",
    "\n",
    "# # Retrieve documents\n",
    "# docs = web_research_retriever.get_relevant_documents(\"Who are competitors of ChatGPT?\")\n",
    "# print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e158a2-6ce1-40f2-af54-bbe5929fbe85",
   "metadata": {},
   "source": [
    "# 3. Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4188d-74b2-44e1-beb5-ee54c05e4c9d",
   "metadata": {},
   "source": [
    "The chains are responsible for creating an end-to-end pipeline for using the language models. They will join the model, prompt, memory, parsing output, and debugging capability and provide an easy-to-use interface. A chain will \n",
    "1. receive the user’s query as an input, \n",
    "2. process the LLM’s response, and lastly, \n",
    "3. return the output to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae745fd-c7f2-456b-8b82-3a3cd4cf0ecb",
   "metadata": {},
   "source": [
    "## LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8339c6a-0ca2-4067-a0fb-1f33ab501e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Intelligence\\n\\n1. Machine Learning\\n2. Cognitive Computing\\n3. Robotic Intelligence\\n4. Automated Intelligence\\n5. Smart Technology']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "csv_parser = CommaSeparatedListOutputParser()\n",
    "template = \"\"\"List five best words as substitute for {word} as comma separated.\"\"\"\n",
    "prompt=PromptTemplate(template=template, input_variables=['word'])\n",
    "\n",
    "llm_chain = LLMChain(llm=llm_openai, prompt=prompt, output_parser=csv_parser)\n",
    "\n",
    "llm_chain.predict(word= \"Artificial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36c84b1c-7b79-42ae-a863-697e165d9fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No sé cuál es tu nombre o dónde vives o cualquier cosa en absoluto.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_template = \"\"\"\n",
    "Translate the following text from {source_language} to {target_language}: \n",
    "{text}:\n",
    "translated_text:\n",
    "\"\"\"\n",
    "translation_prompt = PromptTemplate(template=translation_template, input_variables=[\"source_language\", \"target_language\", \"text\"])\n",
    "translation_chain = LLMChain(llm=llm_openai, prompt=translation_prompt)\n",
    "source_language = \"English\"\n",
    "target_language = \"Spanish\" # \"Hindi\"\n",
    "text = \"I don't know what your name is or where do you live or anything whatsoever.\"\n",
    "translated_text = translation_chain.predict(source_language=source_language, target_language=target_language, text=text)\n",
    "translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1c7ba11-eb29-442f-856e-f1ca925676f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital city of France is Paris.\n",
      "The largest mammal on Earth is the blue whale.\n",
      " Nitrogen (N2) is the most abundant gas in Earth's atmosphere, making up about 78% of the total volume. Oxygen (O2) is the second most abundant gas, making up about 21% of the total volume. Other gases, such as argon, carbon dioxide, and water vapor, make up the remaining 1%.\n",
      "A ripe banana is typically yellow in color.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain = LLMChain( prompt=prompt, llm=llm_openai)\n",
    "\n",
    "\n",
    "qa = [\n",
    "    {'question': \"What is the capital city of France?\"},\n",
    "    {'question': \"What is the largest mammal on Earth?\"},\n",
    "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question': \"What color is a ripe banana?\"}\n",
    "]\n",
    "res = llm_chain.generate(qa)\n",
    "for res_one in res.generations:\n",
    "    print(res_one[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a315bd5-47a5-4d24-beac-4b6ffcc744a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='\\n\\nSynthetic, Man-made, Simulated, Faux, Imitation', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n1. Conduct\\n2. Manner\\n3. Demeanor\\n4. Attitude\\n5. Disposition', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n1. Clever, \\n2. Resourceful, \\n3. Astute, \\n4. Ingenious, \\n5. Discerning', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'prompt_tokens': 36, 'completion_tokens': 71, 'total_tokens': 107}, 'model_name': 'gpt-3.5-turbo-instruct'} run=[RunInfo(run_id=UUID('46319bc1-6cce-4e71-8fd4-e74f7f189b26')), RunInfo(run_id=UUID('21a3ec52-38f5-499f-8e6d-9a2d37ac04cb')), RunInfo(run_id=UUID('8a5aff9b-c4bd-4b73-b477-184fb9938ace'))]\n"
     ]
    }
   ],
   "source": [
    "# word_list= [{\"word\": \"Artificial\"}, {\"word\": \"Behaviour\"}, {\"word\": \"Intelligent\"}]\n",
    "# response= llm_chain.generate(word_list)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8176ebd8-71f0-49e4-a047-0da1b3fe8d0b",
   "metadata": {},
   "source": [
    "## ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "941af9ce-7977-4d62-a073-9f0c8c88516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Synthetic', 'simulated', 'man-made', 'fabricated', 'ersatz.']\n",
      "[\"'Imitation'\", \"'virtual'\", \"'constructed'\", \"'replicated.'\"]\n"
     ]
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=llm_openai, memory=ConversationBufferMemory(), output_parser=csv_parser)\n",
    "\n",
    "response= conversation.predict(input=\"List five best words as substitute for 'artificial' as comma separated.\")\n",
    "print(response)\n",
    "\n",
    "response= conversation.predict(input=\"And the next 4 words.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e87341-2d3b-454d-a6ac-ca3764ff8cea",
   "metadata": {},
   "source": [
    "## Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "858251a6-5234-498d-b74b-879da32e9c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "poem: Love between two troubled souls,\n",
      "A bond that's strong, yet full of holes.\n",
      "They've both been hurt, they've both been scarred,\n",
      "But somehow, together, they've found a shard.\n",
      "\n",
      "They understand each other's pain,\n",
      "And know that love can heal again.\n",
      "They hold each other through the night,\n",
      "And chase away each other's fright.\n",
      "\n",
      "Their love is not a fairytale,\n",
      "But it's real and raw, and it prevails.\n",
      "They've been through storms, they've been through fire,\n",
      "But their love only grows higher.\n",
      "\n",
      "They may not have it all figured out,\n",
      "But they know what love is all about.\n",
      "It's not perfect, it's not easy,\n",
      "But it's worth it, and it's oh so freeing.\n",
      "\n",
      "Their troubled souls may never fully heal,\n",
      "But together, they've found something real.\n",
      "Love between two troubled souls,\n",
      "A bond that's strong, and it only grows.\n",
      "\n",
      "Romantic_expressions: True\n",
      "Nature_descriptions: False\n"
     ]
    }
   ],
   "source": [
    "# poet\n",
    "poet_template = \"\"\"You are an American poet, your job is to come up with\\\n",
    "poems based on a given theme.\n",
    "\n",
    "Here is the theme you have been asked to generate a poem on:\n",
    "{input}\\\n",
    "\"\"\"\n",
    "\n",
    "poet_prompt_template = PromptTemplate(template=poet_template, input_variables=[\"input\"])\n",
    "\n",
    "# creating the poet chain\n",
    "poet_chain = LLMChain(llm=llm_openai, prompt=poet_prompt_template, output_key=\"poem\")\n",
    "\n",
    "# critic\n",
    "critic_template = \"\"\"You are a critic of poems, you are tasked\\\n",
    "to inspect the themes of poems. Identify whether the poem includes romantic expressions or descriptions of nature.\n",
    "\n",
    "Your response should be in the following format, as a Python Dictionary.\n",
    "poem: this should be the poem you received \n",
    "Romantic_expressions: True or False\n",
    "Nature_descriptions: True or False\n",
    "\n",
    "Here is the poem submitted to you:\n",
    "{poem}\\\n",
    "\"\"\"\n",
    "\n",
    "critic_prompt_template = PromptTemplate(template=critic_template, input_variables=[\"poem\"])\n",
    "\n",
    "# creating the critic chain\n",
    "critic_chain= LLMChain(llm=llm_openai, prompt=critic_prompt_template, output_key=\"critic_verified\")\n",
    "\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[poet_chain, critic_chain])\n",
    "\n",
    "# Run the poet and critic chain with a specific theme\n",
    "theme = \"Love between two troubled souls\"\n",
    "review = overall_chain.run(theme)\n",
    "\n",
    "# Print the review to see the critic's evaluation\n",
    "print(review)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a559e449-e4e4-4677-a140-87a219300eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Day 1: Arrival in Ulaanbaatar, the capital city of Mongolia. Explore the city's main attractions such as the Gandan Monastery, Sukhbaatar Square, and the National Museum of Mongolia.\n",
      "2. Day 2: Take a scenic drive to Lake Khuvsgul, known as the \"Blue Pearl of Mongolia\". Enjoy a boat ride on the lake and take in the breathtaking views of the surrounding mountains.\n",
      "3. Day 3: Visit Terkhiin Tsagaan Lake, also known as the \"Great White Lake\". Take a hike around the lake and enjoy a picnic lunch while admiring the stunning landscape.\n",
      "4. Day 4: Head to the Orkhon River and spend the day fishing or rafting on its crystal clear waters. In the evening, experience a traditional Mongolian dinner and cultural performance.\n",
      "5. Day 5: Return to Ulaanbaatar and spend the day shopping for souvenirs and trying out local cuisine before departing for your next destination.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CSV parser\n",
    "csv_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "suggestions_template = \"\"\"\n",
    "You are an assistant that helps users to plan trips.\n",
    "Suggest the best place to visit in {country} for someone who loves beach.\n",
    "\"\"\"\n",
    "\n",
    "suggestions_prompt = PromptTemplate(\n",
    "    template=suggestions_template,\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "# creating the suggestions chain\n",
    "suggestions_chain = LLMChain(llm=llm_openai, prompt=suggestions_prompt, output_key=\"place\")\n",
    "\n",
    "itinerary_template = \"\"\"You are an assistant that helps users to plan trips.\n",
    "Give a brief itinerary for {place} of five days as a numbered list.\n",
    "Itinerary:\n",
    "\"\"\"\n",
    "\n",
    "itinerary_prompt = PromptTemplate(template=itinerary_template, input_variables=[\"place\"])\n",
    "\n",
    "# creating the itinerary chain\n",
    "itinerary_chain= LLMChain(llm=llm_openai, prompt=itinerary_prompt, output_key=\"detailed_itinerary\")\n",
    "\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[suggestions_chain, itinerary_chain])\n",
    "\n",
    "country= \"Mongolia\"\n",
    "itinerary = overall_chain.run(country)\n",
    "\n",
    "# Print the review to see the critic's evaluation\n",
    "print(itinerary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb57fb0-cfbc-4e5c-8c60-a67183516fe8",
   "metadata": {},
   "source": [
    "## RetrievalQA Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8ffc1-a76a-46b4-b1be-a7dbbdbc64a7",
   "metadata": {},
   "source": [
    "This chain first does a retrieval step to fetch relevant documents, then passes those documents into an LLM to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26b0d798-04a8-45b1-a697-3603c02d80dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 12 documents\n",
      "There are 12 in the collection\n",
      "\n",
      "Google plans to challenge OpenAI by opening up its AI language model PaLM and launching an API for it, along with other AI enterprise tools.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20, length_function=len)\n",
    "rcts_docs = text_splitter.split_documents(text_doc)\n",
    "print (f\"You have {len(rcts_docs)} documents\")\n",
    "# print (rcts_docs)\n",
    "\n",
    "\n",
    "openai_embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "chromadb_client = chromadb.Client()\n",
    "db_openai_embed = Chroma.from_documents(\n",
    "    rcts_docs, openai_embed_model, client=chromadb_client, collection_name=\"tmp_collection\"\n",
    ")\n",
    "\n",
    "print(\"There are\", db_openai_embed._collection.count(), \"in the collection\")\n",
    "\n",
    "retriever = db_openai_embed.as_retriever()\n",
    "\n",
    "# create a retrieval chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm= llm_openai, chain_type=\"stuff\", retriever=retriever)\n",
    "query = \"How Google plans to challenge OpenAI?\"\n",
    "response = qa_chain.run(query)\n",
    "print(response)\n",
    "chromadb_client.delete_collection(name=\"tmp_collection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0fc962e-3e2c-414a-8988-9c891138685e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1z/nk87pzz50cx3xjssc8v1lgx40000gq/T/ipykernel_505/3482602296.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mqa_chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'qa_chain' is not defined"
     ]
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1657ea49-797e-4ed4-bc48-891ddf94652f",
   "metadata": {},
   "source": [
    "### RetrievalQAWithSourcesChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51caa48d-acd1-40b4-9a5e-98dc12ef01a4",
   "metadata": {},
   "source": [
    "It does question answering over retrieved documents, and cites it sources. Use this when you want the answer response to have sources in the text response. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f42bf-d007-4004-898e-fbbe7910ed4d",
   "metadata": {},
   "source": [
    "There are many more chains such as, __TransformationChain, LLMCheckerChain, LLMSummarizationChain, Custom Chain, etc__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10450356-15c6-411e-ba47-7fcd0fd13726",
   "metadata": {},
   "source": [
    "# 4. Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad6614f-0b69-463f-87e1-8bee59b15f7c",
   "metadata": {},
   "source": [
    "In the ever-evolving world of chatbot applications, maintaining message history can be essential for delivering context-aware responses that enhance user experiences.\n",
    "\n",
    "In LangChain, memory is a fundamental aspect of conversational interfaces, allowing systems to reference past interactions. This is achieved through storing and querying information, with two primary actions: reading and writing. The memory system interacts with a chain twice during a run, augmenting user inputs and storing the inputs and outputs for future reference.\n",
    "\n",
    "\n",
    "* __Storing Chat Messages__: The LangChain memory module integrates various methods to store chat messages, ranging from in-memory lists to databases. This ensures that all chat interactions are recorded for future reference.\n",
    "* __Querying Chat Messages__: Beyond storing chat messages, LangChain employs data structures and algorithms to create a useful view of these messages. Simple memory systems might return recent messages, while more advanced systems could summarize past interactions or focus on entities mentioned in the current interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84829c85-4f8a-4bff-82dc-a02bddc5c278",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878c160-4eec-452c-8246-b12cf3d0e466",
   "metadata": {},
   "source": [
    "This memory implementation stores the entire conversation history as a single string. The advantages of this approach is maintains a complete record of  the conversation, as well as being straightforward to implement and use. On the other hands, It can be less efficient as the conversation grows longer and may lead to excessive repetition if the conversation history is too long for the model's token limit.\n",
    "\n",
    "If the token limit of the model is surpassed, the buffer gets truncated to fit within the model's token limit. This means that older interactions may be removed from the buffer to accommodate newer ones, and as a result, the conversation context might lose some information.\n",
    "\n",
    "To avoid surpassing the token limit, you can monitor the token count in the buffer and manage the conversation accordingly. For example, you can choose to shorten the input texts or remove less relevant parts of the conversation to keep the token count within the model's limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fdaa612-8bdd-4fcc-a573-5ae00336c8e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=llm_openai, verbose=True)\n",
    "# conversation = ConversationChain(llm=llm_openai, verbose=True, memory=ConversationBufferMemory())\n",
    "output = conversation.predict(input=\"Hi there!\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06bb73f3-c809-46fd-a52a-18ec5b84fb95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?\n",
      "Human: What is the largest retail company in the world?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?\n",
      "Human: What is the largest retail company in the world?\n",
      "AI:  According to Forbes, the largest retail company in the world is Walmart, with a revenue of over $500 billion in 2018. However, it is worth noting that Alibaba, a Chinese e-commerce company, surpassed Walmart in terms of total sales in 2016. Is there anything else you would like to know?\n",
      "Human: Who is the CEO of the company and where is the headquarter?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?\n",
      "Human: What is the largest retail company in the world?\n",
      "AI:  According to Forbes, the largest retail company in the world is Walmart, with a revenue of over $500 billion in 2018. However, it is worth noting that Alibaba, a Chinese e-commerce company, surpassed Walmart in terms of total sales in 2016. Is there anything else you would like to know?\n",
      "Human: Who is the CEO of the company and where is the headquarter?\n",
      "AI:  The current CEO of Walmart is Doug McMillon, who has been in the position since 2014. The company's headquarters is located in Bentonville, Arkansas, in the United States. Is there anything else you would like to know?\n",
      "Human: Who are the competitors?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Some of Walmart's main competitors include Amazon, Costco, Target, and Kroger. However, Walmart also competes with smaller regional and local retailers in different markets. Is there anything else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "output = conversation.predict(input=\"What is the largest retail company in the world?\")\n",
    "output = conversation.predict(input=\"Who is the CEO of the company and where is the headquarter?\")\n",
    "output = conversation.predict(input=\"Who are the competitors?\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50852deb-7f0d-4c10-915f-f899eae97e9a",
   "metadata": {},
   "source": [
    "The __ConversationChain__ uses the __ConversationBufferMemory__ class by default to provide a history of messages. This memory can save the previous conversations in form of variables. The class accepts the ___return_messages___ argument which is helpful for dealing with chat models. This is how the CoversationChain keep context under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26cc8422-ec4b-476a-9327-5b16b43d9ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content='hi there!'), AIMessage(content=\"Hi there! It's nice to meet you. How can I help you today?\")]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\": \"hi there!\"}, {\"output\": \"Hi there! It's nice to meet you. How can I help you today?\"})\n",
    "\n",
    "print( memory.load_memory_variables({}) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9078744-8784-4380-8b67-cfda6fab79ac",
   "metadata": {},
   "source": [
    "Here we used __MessagesPlaceholder__ function to create a placeholder for the conversation history in a chat model prompt. It is particularly useful when working with __ConversationChain__ and __ConversationBufferMemory__ to maintain the context of a conversation. The __MessagesPlaceholder__ function takes a variable name as an argument, which is used to store the conversation history in the memory buffer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28c11834-8ef3-43ab-be61-be964aeb32c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between a human and an AI.\n",
      "Human: What was the most important research paper published in the field of Natural Language Processing?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'What was the most important research paper published in the field of Natural Language Processing?', 'history': [HumanMessage(content='What was the most important research paper published in the field of Natural Language Processing?'), AIMessage(content=' \\n\\nAI: According to my database, the most important research paper published in the field of Natural Language Processing is \"Attention is All You Need\" by Vaswani et al. This paper introduced the Transformer model, which has greatly advanced the field of NLP and has been used in various applications such as machine translation and text summarization.')], 'response': ' \\n\\nAI: According to my database, the most important research paper published in the field of Natural Language Processing is \"Attention is All You Need\" by Vaswani et al. This paper introduced the Transformer model, which has greatly advanced the field of NLP and has been used in various applications such as machine translation and text summarization.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(llm=llm_openai, memory=memory, prompt=prompt, verbose=True)\n",
    "\n",
    "user_message = \"What was the most important research paper published in the field of Natural Language Processing?\"\n",
    "response = conversation(user_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96c5c0cb-5f92-4dbf-b483-b533f6d0b8e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between a human and an AI.\n",
      "Human: What was the most important research paper published in the field of Natural Language Processing?\n",
      "AI:  \n",
      "\n",
      "AI: According to my database, the most important research paper published in the field of Natural Language Processing is \"Attention is All You Need\" by Vaswani et al. This paper introduced the Transformer model, which has greatly advanced the field of NLP and has been used in various applications such as machine translation and text summarization.\n",
      "Human: Please give a brief description of the various building blocks of that paper.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Please give a brief description of the various building blocks of that paper.', 'history': [HumanMessage(content='What was the most important research paper published in the field of Natural Language Processing?'), AIMessage(content=' \\n\\nAI: According to my database, the most important research paper published in the field of Natural Language Processing is \"Attention is All You Need\" by Vaswani et al. This paper introduced the Transformer model, which has greatly advanced the field of NLP and has been used in various applications such as machine translation and text summarization.'), HumanMessage(content='Please give a brief description of the various building blocks of that paper.'), AIMessage(content='\\nAI: The Transformer model introduced in the paper consists of two main building blocks: the self-attention mechanism and the feed-forward neural network. The self-attention mechanism allows the model to focus on different parts of the input sequence, while the feed-forward neural network helps to capture complex relationships between words. Additionally, the paper also introduced the concept of positional encoding, which helps the model to understand the order of words in a sentence. These building blocks have been widely adopted in subsequent NLP research and have greatly improved the performance of language models.')], 'response': '\\nAI: The Transformer model introduced in the paper consists of two main building blocks: the self-attention mechanism and the feed-forward neural network. The self-attention mechanism allows the model to focus on different parts of the input sequence, while the feed-forward neural network helps to capture complex relationships between words. Additionally, the paper also introduced the concept of positional encoding, which helps the model to understand the order of words in a sentence. These building blocks have been widely adopted in subsequent NLP research and have greatly improved the performance of language models.'}\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Please give a brief description of the various building blocks of that paper.\"\n",
    "response = conversation(user_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18508723-7b10-443d-9a57-103804828096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between a human and an AI.\n",
      "Human: What was the most important research paper published in the field of Natural Language Processing?\n",
      "AI:  \n",
      "\n",
      "AI: According to my database, the most important research paper published in the field of Natural Language Processing is \"Attention is All You Need\" by Vaswani et al. This paper introduced the Transformer model, which has greatly advanced the field of NLP and has been used in various applications such as machine translation and text summarization.\n",
      "Human: Please give a brief description of the various building blocks of that paper.\n",
      "AI: \n",
      "AI: The Transformer model introduced in the paper consists of two main building blocks: the self-attention mechanism and the feed-forward neural network. The self-attention mechanism allows the model to focus on different parts of the input sequence, while the feed-forward neural network helps to capture complex relationships between words. Additionally, the paper also introduced the concept of positional encoding, which helps the model to understand the order of words in a sentence. These building blocks have been widely adopted in subsequent NLP research and have greatly improved the performance of language models.\n",
      "Human: When was this paper published?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'When was this paper published?', 'history': [HumanMessage(content='What was the most important research paper published in the field of Natural Language Processing?'), AIMessage(content=' \\n\\nAI: According to my database, the most important research paper published in the field of Natural Language Processing is \"Attention is All You Need\" by Vaswani et al. This paper introduced the Transformer model, which has greatly advanced the field of NLP and has been used in various applications such as machine translation and text summarization.'), HumanMessage(content='Please give a brief description of the various building blocks of that paper.'), AIMessage(content='\\nAI: The Transformer model introduced in the paper consists of two main building blocks: the self-attention mechanism and the feed-forward neural network. The self-attention mechanism allows the model to focus on different parts of the input sequence, while the feed-forward neural network helps to capture complex relationships between words. Additionally, the paper also introduced the concept of positional encoding, which helps the model to understand the order of words in a sentence. These building blocks have been widely adopted in subsequent NLP research and have greatly improved the performance of language models.'), HumanMessage(content='When was this paper published?'), AIMessage(content=' \\nAI: The paper was published in 2017 at the Conference on Neural Information Processing Systems (NeurIPS).')], 'response': ' \\nAI: The paper was published in 2017 at the Conference on Neural Information Processing Systems (NeurIPS).'}\n"
     ]
    }
   ],
   "source": [
    "user_message = \"When was this paper published?\"\n",
    "response = conversation(user_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8429838e-455b-4f52-bb5a-53d078edb936",
   "metadata": {},
   "source": [
    "In a scenario where a conversation has a large sum of tokens, the computational cost and resources required for processing the conversation will be higher. This highlights the importance of managing tokens effectively. Strategies for achieving this include limiting memory size through methods like __ConversationBufferWindowMemory__ or summarizing older interactions using __ConversationSummaryBufferMemory__. These approaches help control the token count while minimizing associated costs and computational demands in a more efficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d46f6c-51b8-4489-8da8-1a8a5e614385",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8201b-8f6d-4dbe-9231-0206bc709b21",
   "metadata": {},
   "source": [
    "This class limits memory size by keeping a list of the most __recent K__ interactions. It maintains a sliding window of these recent interactions, ensuring that the buffer does not grow too large. \n",
    "\n",
    "Basically, this implementation stores a fixed number of recent messages in the conversation that makes it more efficient than __ConversationBufferMemory__.  \n",
    "\n",
    "Also, it reduces the risk of exceeding the model's token limit. However, the downside of using this approach is that it does not maintain the complete conversation history. \n",
    "\n",
    "The chatbot might lose context if essential information falls outside the fixed window of messages.\n",
    "\n",
    "It is possible to retrieve specific interactions from ConversationBufferWindowMemory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74e7a1c8-a954-4509-8dd0-6d2fd2d6a811",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between a human and an AI.\n",
      "Human: What was the most important research paper published in the field of Natural Language Processing?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'What was the most important research paper published in the field of Natural Language Processing?', 'history': [], 'response': ' \\n\\nAI: According to my database, the most important research paper published in the field of Natural Language Processing is \"Attention is All You Need\" by Vaswani et al. This paper introduced the Transformer model, which has greatly advanced the field of NLP and has been used in various applications such as machine translation and text summarization.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=2, return_messages=True)\n",
    "conversation = ConversationChain(llm=llm_openai, memory=memory, prompt=prompt, verbose=True)\n",
    "\n",
    "user_message = \"What was the most important research paper published in the field of Natural Language Processing?\"\n",
    "response = conversation(user_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bde9b492-10af-4446-b78e-3a1f050d65b3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between a human and an AI.\n",
      "Human: What was the most important research paper published in the field of Natural Language Processing?\n",
      "AI:  \n",
      "\n",
      "AI: According to my database, the most important research paper published in the field of Natural Language Processing is \"Attention is All You Need\" by Vaswani et al. This paper introduced the Transformer model, which has greatly advanced the field of NLP and has been used in various applications such as machine translation and text summarization.\n",
      "Human: Please give a brief description of the various building blocks of that paper.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Please give a brief description of the various building blocks of that paper.', 'history': [HumanMessage(content='What was the most important research paper published in the field of Natural Language Processing?'), AIMessage(content=' \\n\\nAI: According to my database, the most important research paper published in the field of Natural Language Processing is \"Attention is All You Need\" by Vaswani et al. This paper introduced the Transformer model, which has greatly advanced the field of NLP and has been used in various applications such as machine translation and text summarization.')], 'response': '\\nAI: The Transformer model introduced in the paper consists of two main building blocks: the self-attention mechanism and the feed-forward neural network. The self-attention mechanism allows the model to focus on different parts of the input sequence, while the feed-forward neural network helps to capture complex relationships between words. Additionally, the paper also introduced the concept of positional encoding, which helps the model to understand the order of words in a sentence. These building blocks have been widely adopted in subsequent NLP research and have greatly improved the performance of language models.'}\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Please give a brief description of the various building blocks of that paper.\"\n",
    "response = conversation(user_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ebd4542-8d7d-4bd3-9114-1609d7cd9da5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between a human and an AI.\n",
      "Human: What was the most important research paper published in the field of Natural Language Processing?\n",
      "AI:  \n",
      "\n",
      "AI: According to my database, the most important research paper published in the field of Natural Language Processing is \"Attention is All You Need\" by Vaswani et al. This paper introduced the Transformer model, which has greatly advanced the field of NLP and has been used in various applications such as machine translation and text summarization.\n",
      "Human: Please give a brief description of the various building blocks of that paper.\n",
      "AI: \n",
      "AI: The Transformer model introduced in the paper consists of two main building blocks: the self-attention mechanism and the feed-forward neural network. The self-attention mechanism allows the model to focus on different parts of the input sequence, while the feed-forward neural network helps to capture complex relationships between words. Additionally, the paper also introduced the concept of positional encoding, which helps the model to understand the order of words in a sentence. These building blocks have been widely adopted in subsequent NLP research and have greatly improved the performance of language models.\n",
      "Human: When was this paper published?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'When was this paper published?', 'history': [HumanMessage(content='What was the most important research paper published in the field of Natural Language Processing?'), AIMessage(content=' \\n\\nAI: According to my database, the most important research paper published in the field of Natural Language Processing is \"Attention is All You Need\" by Vaswani et al. This paper introduced the Transformer model, which has greatly advanced the field of NLP and has been used in various applications such as machine translation and text summarization.'), HumanMessage(content='Please give a brief description of the various building blocks of that paper.'), AIMessage(content='\\nAI: The Transformer model introduced in the paper consists of two main building blocks: the self-attention mechanism and the feed-forward neural network. The self-attention mechanism allows the model to focus on different parts of the input sequence, while the feed-forward neural network helps to capture complex relationships between words. Additionally, the paper also introduced the concept of positional encoding, which helps the model to understand the order of words in a sentence. These building blocks have been widely adopted in subsequent NLP research and have greatly improved the performance of language models.')], 'response': ' \\nAI: The paper was published in 2017 at the Conference on Neural Information Processing Systems (NeurIPS).'}\n"
     ]
    }
   ],
   "source": [
    "user_message = \"When was this paper published?\"\n",
    "response = conversation(user_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06c092f2-3bda-4937-b18b-d6be0315ad1f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between a human and an AI.\n",
      "Human: Please give a brief description of the various building blocks of that paper.\n",
      "AI: \n",
      "AI: The Transformer model introduced in the paper consists of two main building blocks: the self-attention mechanism and the feed-forward neural network. The self-attention mechanism allows the model to focus on different parts of the input sequence, while the feed-forward neural network helps to capture complex relationships between words. Additionally, the paper also introduced the concept of positional encoding, which helps the model to understand the order of words in a sentence. These building blocks have been widely adopted in subsequent NLP research and have greatly improved the performance of language models.\n",
      "Human: When was this paper published?\n",
      "AI:  \n",
      "AI: The paper was published in 2017 at the Conference on Neural Information Processing Systems (NeurIPS).\n",
      "Human: Who were the authors of this paper?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Who were the authors of this paper?', 'history': [HumanMessage(content='Please give a brief description of the various building blocks of that paper.'), AIMessage(content='\\nAI: The Transformer model introduced in the paper consists of two main building blocks: the self-attention mechanism and the feed-forward neural network. The self-attention mechanism allows the model to focus on different parts of the input sequence, while the feed-forward neural network helps to capture complex relationships between words. Additionally, the paper also introduced the concept of positional encoding, which helps the model to understand the order of words in a sentence. These building blocks have been widely adopted in subsequent NLP research and have greatly improved the performance of language models.'), HumanMessage(content='When was this paper published?'), AIMessage(content=' \\nAI: The paper was published in 2017 at the Conference on Neural Information Processing Systems (NeurIPS).')], 'response': '\\nAI: The authors of this paper were Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.'}\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Who were the authors of this paper?\"\n",
    "response = conversation(user_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "538754d0-89aa-4862-b571-179fadcabb1d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The following is a friendly conversation between a human and an AI.\n",
      "Human: Who were the authors of this paper?\n",
      "AI: \n",
      "AI: The authors of this paper were Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.\n",
      "Human: What was the name of the paper you had mentioned?\n",
      "AI: \n",
      "AI: The name of the paper is \"Attention is All You Need\".\n",
      "Human: What was the first question I asked?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'What was the first question I asked?', 'history': [HumanMessage(content='Who were the authors of this paper?'), AIMessage(content='\\nAI: The authors of this paper were Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.'), HumanMessage(content='What was the name of the paper you had mentioned?'), AIMessage(content='\\nAI: The name of the paper is \"Attention is All You Need\".')], 'response': '\\nAI: Your first question was \"Who were the authors of this paper?\"'}\n"
     ]
    }
   ],
   "source": [
    "user_message = \"What was the first question I asked?\"\n",
    "response = conversation(user_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbdd157-a7ba-4423-ae6f-849b63439a60",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf65541-2930-4c1b-b080-f3a064bbae86",
   "metadata": {},
   "source": [
    "__ConversationSummaryMemory__ is a memory management strategy that reates a summary of the conversation over time, useful for condensing information from longer conversations. \n",
    "\n",
    "It extracts key information from previous interactions and condenses it into a shorter, more manageable format.  \n",
    "\n",
    "Here is a list of pros and cons of ConversationSummaryMemory.\n",
    "\n",
    "* __Pros__:\n",
    "\n",
    "    * __Condensing conversation information__: By summarizing the conversation, it helps reduce the number of tokens required to store the conversation history, which can be beneficial when working with token-limited models like GPT-3\n",
    "    * __Flexibility__: You can configure this type of memory to return the history as a list of messages or as a plain text summary. This makes it suitable for chatbots.\n",
    "    * __Direct summary prediction__: The predict_new_summary method allows you to directly obtain a summary prediction based on the list of messages and the previous summary. This enables you to have more control over the summarization process.\n",
    "\n",
    "\n",
    "* __Cons__:\n",
    "\n",
    "    * __Loss of information__: Summarizing the conversation might lead to a loss of information, especially if the summary is too short or omits important details from the conversation.\n",
    "    * __Increased complexity__: Compared to simpler memory types like ConversationBufferMemory, which just stores the raw conversation history, ConversationSummaryMemoryrequires more processing to generate the summary, potentially affecting the performance of the chatbot. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5368cfc2-9a35-43b1-97b5-d5a7e74ec97f",
   "metadata": {},
   "source": [
    "## ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37175458-0943-4171-8266-c27d64990d62",
   "metadata": {},
   "source": [
    "__ConversationSummaryBufferMemory__ is a memory management strategy that combines the ideas of keeping a buffer of recent interactions in memory and compiling old interactions into a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c75ca1ea-9bbb-40c6-bb5d-72e09366f0b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory=ConversationSummaryBufferMemory(llm=llm_openai, max_token_limit=40)\n",
    "conversation_with_summary = ConversationChain(llm=llm_openai, memory=memory, verbose=True)\n",
    "output = conversation_with_summary.predict(input=\"Hi there!\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "026cb000-bbad-429f-b95e-f3c744f25074",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and the AI responds by introducing itself as an AI created by OpenAI. The AI explains that it is constantly learning and improving through machine learning algorithms and asks how it can assist the human.\n",
      "Human: What is the largest retail company in the world?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and the AI responds by introducing itself as an AI created by OpenAI. The AI explains that it is constantly learning and improving through machine learning algorithms and asks how it can assist the human. The human asks about the largest retail company in the world and the AI provides information on Walmart's revenue in 2018, but notes that the retail industry is constantly evolving. The AI offers further assistance.\n",
      "Human: Who is the CEO of the company and where is the headquarter?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and the AI responds by introducing itself as an AI created by OpenAI. The AI explains that it is constantly learning and improving through machine learning algorithms and asks how it can assist the human. The human asks about the largest retail company in the world and the AI provides information on Walmart's revenue in 2018, but notes that the retail industry is constantly evolving. The AI offers further assistance. The human asks who the CEO of the company is and where the headquarters is located.\n",
      "AI:  The current CEO of Walmart is Doug McMillon, and the company's headquarters is located in Bentonville, Arkansas, United States.\n",
      "Human: Who are the competitors?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Some of Walmart's main competitors include Amazon, Target, and Costco. However, the retail industry is constantly evolving and new competitors may emerge in the future. Is there anything else I can assist you with?\n"
     ]
    }
   ],
   "source": [
    "output = conversation_with_summary.predict(input=\"What is the largest retail company in the world?\")\n",
    "output = conversation_with_summary.predict(input=\"Who is the CEO of the company and where is the headquarter?\")\n",
    "output = conversation_with_summary.predict(input=\"Who are the competitors?\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22855fd0-55be-4825-938e-28656aa4c291",
   "metadata": {},
   "source": [
    "## VectorStoreRetrieverMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94113f25-a442-460a-9342-cc5b010b8cb2",
   "metadata": {},
   "source": [
    "VectorStoreRetrieverMemory stores memories in a vector store and queries the top-K most \"salient\" documents every time it is called. This memory type doesn't explicitly track the order of interactions but uses vector retrieval to fetch relevant memories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b12e7a59-3406-4368-91de-084a5837b5c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize your vector store (specifics depend on the chosen vector store)\n",
    "import faiss\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedding_size = 1536  # Dimensions of the OpenAIEmbeddings\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "embedding_fn = OpenAIEmbeddings().embed_query\n",
    "vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})\n",
    "\n",
    "# Create your VectorStoreRetrieverMemory\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=1))\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "# Save context and relevant information to the memory\n",
    "memory.save_context({\"input\": \"My favorite food is pizza\"}, {\"output\": \"that's good to know\"})\n",
    "memory.save_context({\"input\": \"My favorite sport is soccer\"}, {\"output\": \"...\"})\n",
    "memory.save_context({\"input\": \"I don't like the Celtics\"}, {\"output\": \"ok\"})\n",
    "\n",
    "# Retrieve relevant information from memory based on a query\n",
    "print(memory.load_memory_variables({\"prompt\": \"what sport should i watch?\"})[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4785e5ad-ad62-4cdf-aa48-47415577a1bc",
   "metadata": {},
   "source": [
    "# 5. Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8400a3-5756-438b-80a6-b4f5d7eb68c8",
   "metadata": {},
   "source": [
    "Tools are instrumental in connecting the language model with other sources of data or computation, including search engines, APIs, and other data repositories. Language models can only access the knowledge they've been trained on, which can quickly become obsolete.\n",
    "\n",
    "Tools are modular, reusable components meticulously designed to accomplish specific tasks or provide answers to distinct types of questions. By integrating these tools seamlessly into the system, users can effortlessly tap into a diverse range of functionalities and information sources to tackle challenges and generate meaningful responses.\n",
    "\n",
    "\n",
    "A few notable examples of tools in LangChain are:\n",
    "* __Google Search__: This tool uses the Google Search API to fetch relevant information from the web, which can be used to answer queries related to current events, facts, or any topic where a quick search can provide accurate results.\n",
    "* __Requests__: This tool employs the popular Python library \"requests\" to interact with web services, access APIs, or obtain data from different online sources. It can be particularly useful for gathering structured data or specific information from a web service.\n",
    "* __SerpAPI__: This tool is used to scrape results from Google and other search engines. It is the JSON representative of Google search result and other search engine.\n",
    "\n",
    "* __Python REPL__: The Python REPL (Read-Eval-Print Loop) tool allows users to execute Python code on-the-fly to perform calculations, manipulate data, or test algorithms. It serves as an interactive programming environment within the LangChain system.\n",
    "* __Wikipedia__: The Wikipedia tool leverages the Wikipedia API to search and retrieve relevant articles, summaries, or specific information from the vast repository of knowledge on the Wikipedia platform.\n",
    "* __Wolfram Alpha__: With this tool, users can tap into the powerful computational knowledge engine of Wolfram Alpha to answer complex questions, perform advanced calculations, or generate visual representations of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba30922-9663-4dab-a632-deccc98f9af5",
   "metadata": {},
   "source": [
    "### GoogleSearchAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5b6c3b05-0505-4dfb-b49f-376d7c05584f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Introduction | 🦜️ Langchain',\n",
       "  'link': 'https://python.langchain.com/docs/get_started/introduction',\n",
       "  'snippet': 'LangChain is a framework for developing applications powered by language models. It enables applications that:'},\n",
       " {'title': 'What Is LangChain and How to Use It: A Guide',\n",
       "  'link': 'https://www.techtarget.com/searchenterpriseai/definition/LangChain',\n",
       "  'snippet': 'Why is LangChain important? LangChain is a framework that simplifies the process of creating generative AI application interfaces. Developers working on these\\xa0...'}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search= GoogleSearchAPIWrapper()\n",
    "search.results(\"Who is the CEO of Walmart?\", 3)\n",
    "# search.results(\"What is Langchain?\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693d3bd-3fbf-4a82-9e0f-afcfcd78d84b",
   "metadata": {},
   "source": [
    "### SerpAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ca1450c9-0e66-4b05-85a5-2321ebf79727",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doug McMillon\n",
      "[\"LangChain is a framework designed to simplify the creation of applications using large language models. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\", 'LangChain entity_type: kp3_verticals.', 'LangChain kgmid: /g/11kjpl7_60.', 'LangChain initial_release_date: October 2022.', 'LangChain developer_s: Harrison Chase.', 'LangChain license: MIT License.', 'LangChain repository: github.com/langchain-ai/langchain.', 'LangChain stable_release: 0.1.8 / 19 February 2024; 24 days ago.', 'LangChain written_in: Python and JavaScript.', 'LangChain is a framework for developing applications powered by language models. It enables applications that:', 'LangChain provides AI developers with tools to connect language models with external data sources. It is open-source and supported by an active community.', \"LangChain uses the power of AI large language models combined with data sources to create quite powerful apps. Read how it works and how it's used.\", 'LangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models.', \"Build your app with LangChain. Build context-aware, reasoning applications with LangChain's flexible framework that leverages your company's data and APIs.\", 'LangChain is a sophisticated framework comprising several key components that work in synergy to enhance natural language processing tasks.', 'At its core, LangChain is a framework built around LLMs. We can use it for chatbots, Generative Question-Answering (GQA), summarization, and much more.', 'LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, ...', 'LangChain is a framework for developing applications powered by language models. It enables applications that: Are context-aware: connect a language model ...']\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "search = SerpAPIWrapper()\n",
    "print(search.run(\"Who is the CEO of Walmart?\"))\n",
    "print(search.run(\"What is Langchain?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f742e45e-253f-4d61-9334-82bc2aa2ad0a",
   "metadata": {},
   "source": [
    "### PythonREPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "10d3fafe-1874-4863-8842-dcd2007eeffd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\n",
      "5\n",
      "55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "python_code= \"\"\"\n",
    "def fib(n):\n",
    "    a, b = 0, 1\n",
    "    for i in range(n):\n",
    "        a, b = b, a + b\n",
    "    return a\n",
    "\n",
    "print(fib(5))\n",
    "print(fib(10))\n",
    "\"\"\"\n",
    "print(python_repl.run(\"print(1+1)\"))\n",
    "print(python_repl.run(python_code))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b015c9a-2aa2-4ab1-ab62-2769bf341f53",
   "metadata": {},
   "source": [
    "### WolframAlphaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "614dbb9c-1e3c-4a59-84d5-a0fdb577e872",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assumption: 2 x + 5 = -3 x + 7 \n",
      "Answer: x = 2/5\n"
     ]
    }
   ],
   "source": [
    "from langchain. utilities.wolfram_alpha import WolframAlphaAPIWrapper\n",
    "\n",
    "wolfram = WolframAlphaAPIWrapper()\n",
    "result = wolfram.run(\"What is 2x+5 = -3x + 7?\")\n",
    "print(result)  # Output: 'x = 2/5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a09d651-f96d-43f0-ada6-c1dac338317d",
   "metadata": {},
   "source": [
    "### WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "06bc9fcd-e18c-42f9-9431-6477ac3a85c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Page: Elon Musk\\nSummary: Elon Reeve Musk (; EE-lon; born June 28, 1971) is a businessman and investor. He is the founder, chairman, CEO, and CTO of SpaceX; angel investor, CEO, product architect, and former chairman of Tesla, Inc.; owner, executive chairman, and CTO of X Corp.; founder of the Boring Company and xAI; co-founder of Neuralink and OpenAI; and president of the Musk Foundation. He is one of the wealthiest people in the world, with an estimated net worth of US$190 billion as of March 2024, according to the Bloomberg Billionaires Index, and $195 billion according to Forbes, primarily from his ownership stakes in Tesla and SpaceX.A member of the wealthy South African Musk family, Elon was born in Pretoria and briefly attended the University of Pretoria before immigrating to Canada at age 18, acquiring citizenship through his Canadian-born mother. Two years later, he matriculated at Queen's University at Kingston in Canada. Musk later transferred to the University of Pennsylvania, and received bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University, but dropped out after two days and, with his brother Kimbal, co-founded online city guide software company Zip2. The startup was acquired by Compaq for $307 million in 1999, and that same year Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal.\\nIn October 2002, eBay acquired PayPal for $1.5 billion, and that same year, with $100 million of the money he made, Musk founded SpaceX, a spaceflight services company. In 2004, he became an early investor in electric vehicle manufacturer Tesla Motors, Inc. (now Tesla, Inc.). He became its chairman and product architect, assuming the position of CEO in 2008. In 2006, Musk helped create SolarCity, a solar-energy company that was acquired by Tesla in 2016 and became Tesla Energy. In 2013, he proposed a hyperloop high-speed vactrain transportation system. In 2015, he co-founded OpenAI, a nonprofit artificial intelligence research company. The following year, Musk co-founded Neuralink—a neurotechnology company developing brain–computer interfaces—and the Boring Company, a tunnel construction company. In 2022, he acquired Twitter for $44 billion. He subsequently merged the company into newly created X Corp. and rebranded the service as X the following year. In March 2023, he founded xAI, an artificial intelligence company.\\nMusk has expressed views that have made him a polarizing figure. He has been criticized for making unscientific and misleading statements, including COVID-19 misinformation and antisemitic conspiracy theories. His ownership of Twitter has been similarly controversial, being marked by the laying off of a large number of employees, an increase in hate speech and misinformation and disinformation on the website, as well as changes to Twitter Blue verification. In 2018, the U.S. Securities and Exchange Commission (SEC) sued him, alleging that he had falsely announced that he had secured funding for a private takeover of Tesla. To settle the case, Musk stepped down as the chairman of Tesla and paid a $20 million fine.\\n\\nPage: Twitter under Elon Musk\\nSummary: Elon Musk completed his acquisition of Twitter in October 2022; Musk acted as CEO of Twitter until June 2023 when he was succeeded by Linda Yaccarino. Twitter was then rebranded to X in July 2023. Initially during Musk's tenure, Twitter introduced a series of reforms and management changes; the company reinstated a number of previously banned accounts, reduced the workforce by approximately 80%, closed one of Twitter's three data centers, and largely eliminated the content moderation team, replacing it with the crowd-sourced fact-checking system Community Notes.\\nIn November 2022, Twitter then began offering paid verification checkmarks, followed by removing legacy verification. In December, the Twitter Files were released and a number of journalists suspended from the platform. The foll\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "wikipedia.run(\"Elon Musk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10626ae-502c-4359-98c5-e624d3898c1d",
   "metadata": {},
   "source": [
    "### Shell (bash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3e3ca084-8878-48b0-abb9-2a834eaa48a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing command:\n",
      " [\"echo 'Hello World!'\", 'ls']\n",
      "Hello World!\n",
      "chroma\n",
      "langchain_fundamentals.ipynb\n",
      "llama.cpp\n",
      "llm_101.ipynb\n",
      "models\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import ShellTool\n",
    "\n",
    "shell_tool = ShellTool()\n",
    "\n",
    "result = shell_tool.run({\"commands\": [\"echo 'Hello World!'\", \"ls\"]})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf872cc-ec0b-4e4e-b962-6968d7671387",
   "metadata": {},
   "source": [
    "Complete list of tools can be found here: https://python.langchain.com/docs/integrations/tools/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60dfeb2-4169-4f6c-a100-2385679a87e0",
   "metadata": {},
   "source": [
    "# 6. Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6cd374-d105-4f96-8c2c-893327a79e0f",
   "metadata": {},
   "source": [
    "We can abstract two primary modes of operation to consider when employing an LLM: as a __content generator__ and as a __reasoning engine__.\n",
    "\n",
    "* When used as a __content generator__, the language model is asked to create content entirely from its internal knowledge base. This approach can lead to highly creative outputs but can also result in unverified information or 'hallucinations' due to the model's reliance on pre-trained knowledge.\n",
    "* On the other hand, when functioning as a __reasoning engine__, the Agent acts more as an information manager rather than a creator. In this mode, it is tasked with gathering relevant, accurate information, often aided by external tools. This involves the LLM drawing from similar resources on a given topic and constructing new content by extracting and summarizing the relevant details.\n",
    "\n",
    "\n",
    "LangChain introduces a powerful concept called __Agents__ that takes the idea of chains to a whole new level. Agents leverage language models to dynamically determine sequences of actions to perform, making them incredibly versatile and adaptive. Unlike traditional chains, where actions are hardcoded in code, agents employ language models as reasoning engines to decide which actions to take and in what order.\n",
    "\n",
    "An agent has access to a suite of tools and can decide which of these tools to call, depending on the user input. Tools are functions that perform specific duties. To create an agent in LangChain, you can use the __initialize_agent__ function along with the __load_tools__ function to prepare the tools the agent can use. \n",
    "\n",
    "The Agent is the core component responsible for decision-making. It harnesses the power of a language model and a prompt to determine the next steps to achieve a specific objective. The inputs to an agent typically include:\n",
    "\n",
    "* __Tools__: Descriptions of available tools .\n",
    "* __User Input__: The high-level objective or query from the user.\n",
    "* __Intermediate Steps__: A history of (action, tool output) pairs executed to reach the current user input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634d6eb5-bc07-4f24-875b-381860f9ea07",
   "metadata": {},
   "source": [
    "In the context of language models, agents are used to decide the course of action and the sequence of these actions. These actions can either be the utilization of a tool, observing its output, or offering a response to the user. The real potential of agents unfolds when they are utilized appropriately. This explanation aims to simplify the usage of agents via the highest-level API.\n",
    "\n",
    "Presently, most of the agents in LangChain fall into one of these two categories:\n",
    "\n",
    "__Action Agents__: These agents determine and execute a single action. They are typically used for straightforward tasks.\n",
    "__Plan-and-Execute Agents__: These agents first devise a plan comprising multiple actions and then execute each action sequentially. They are more suited for complex or long-running tasks as they help maintain focus on long-term objectives.\n",
    "While Action Agents are more traditional and suitable for smaller tasks, Plan-and-Execute Agents help maintain long-term objectives and focus. However, they might lead to more calls and higher latency. Often, it's beneficial to let an Action Agent manage the execution for the Plan-and-Execute agent, thus utilizing both strengths.\n",
    "\n",
    "For example, a high-level workflow of Action Agents would look something like this:\n",
    "\n",
    "* The agent receives user input.\n",
    "* It decides which tool to use (if any) and determines its input.\n",
    "* The chosen tool is called with the provided input, and an observation (the output of the tool) is recorded.\n",
    "* The history of the tool, tool input, and observation are relayed back to the agent, which then decides the next step.\n",
    "* This process is repeated until the agent no longer needs to use a tool, at which point it directly responds to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4d2310d3-cd1f-4f39-9402-232a17ef63ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use wolfram_alpha to get the answer.\n",
      "Action: wolfram_alpha\n",
      "Action Input: 1000 + total number of goals scored in men's football world cup 2022\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mWolfram Alpha wasn't able to answer it\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should try using Search instead.\n",
      "Action: Search\n",
      "Action Input: \"1000 + total number of goals scored in men's football world cup 2022\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mA total of 172 goals were scored during the 2022 World Cup in Qatar, marking a new record for the tournament. This was three more goals than the previous tournament in 2018.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The result of 1000 plus the total number of goals scored in the men's football world cup in 2022 is 1172.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The result of 1000 plus the total number of goals scored in the men's football world cup in 2022 is 1172.\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary modules\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Loading some tools to use. The llm-math tool uses an LLM, so we pass that in.\n",
    "tools = load_tools([\"serpapi\", \"wolfram-alpha\"])\n",
    "\n",
    "# Initializing an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm_openai, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Testing the agent\n",
    "query = \"What's the result of 1000 plus the total number of goals scored in the men's football world cup in 2022?\"\n",
    "response = agent.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475faeb4-f65b-4f0f-ac74-5c8bb30b0b5c",
   "metadata": {},
   "source": [
    "Let's break down the steps to see how the agent functions as a \"reasoning engine\":\n",
    "\n",
    "* __Query Processing__: The agent receives a query: \"What's the result of 1000 plus the total number of goals scored in the men's football world cup in 2022?” The agent identifies two distinct tasks within this query - finding out the number of goals scored in the 2018 soccer world cup and adding 1000 to such number.\n",
    "* __Tool Utilization__: The agent uses the \"serpapi\" or \"google-search\" tool to answer the first part of the query. This is an example of the agent using external tools to gather accurate and relevant information. The agent isn't creating this information; it's pulling the data from an external source.\n",
    "* __Information Processing__: For the second part of the query, the agent uses the \"wolfram-alpha\" tool to perform a sum reliably. Again, the agent isn't creating new information. Instead, it's processing the data it has gathered.\n",
    "* __Synthesis and Response__: After gathering and processing the information, the agent synthesizes it into a coherent response that answers the original query.\n",
    "\n",
    "In this way, the agent acts as a __\"reasoning engine\"__. It's not generating content from scratch but rather gathering, processing, and synthesizing existing information to generate a response.  This approach allows the agent to provide accurate and relevant responses, making it a powerful tool for tasks that involve data retrieval and processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1172d5-3220-4067-9066-ccb7d56aea78",
   "metadata": {},
   "source": [
    "### Python-REPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0c788ec-2406-49d4-9c51-1de274b81101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initialize_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m repl_tool \u001b[38;5;241m=\u001b[39m Tool(\n\u001b[1;32m      7\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython_repl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     func\u001b[38;5;241m=\u001b[39mpython_repl\u001b[38;5;241m.\u001b[39mrun,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m tools\u001b[38;5;241m=\u001b[39m [repl_tool]\n\u001b[0;32m---> 12\u001b[0m agent \u001b[38;5;241m=\u001b[39m initialize_agent(tools, llm_openai, agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat-zero-shot-react-description\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, handle_parsing_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m query\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a code to generate a list of all prime numbers between 1 to 100 and then sort it in descedning order and then print the list.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m response\u001b[38;5;241m=\u001b[39m  agent\u001b[38;5;241m.\u001b[39mrun(query) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'initialize_agent' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "python_repl = PythonREPL()\n",
    "# You can create the tool to pass to an agent\n",
    "repl_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "    func=python_repl.run,\n",
    ")\n",
    "tools= [repl_tool]\n",
    "agent = initialize_agent(tools, llm_openai, agent=\"chat-zero-shot-react-description\", verbose=True, handle_parsing_errors=True)\n",
    "query= \"Write a code to generate a list of all prime numbers between 1 to 100 and then sort it in descedning order and then print the list.\"\n",
    "response=  agent.run(query) \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "19ca3261-30e9-4b45-ab39-857433efb0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use the Search tool to find information about Walmart's history.\n",
      "Action: Search\n",
      "Action Input: \"History of Walmart\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mthe 1960s. Retail Revolution. Sam Walton's strategy is built on an unshakeable foundation: the lowest prices anytime, anywhere. 1962. On July 2, 1962, Sam ... On July 2, 1962, Sam Walton opened the first Walmart store in Rogers, AR. By 1969, the company was officially incorporated and registering $12.7 billion in ... Jan 26, 2024 ... Walmart is an American operator of discount stores; it is one of the world's biggest retailers and among the world's largest corporations. Jan 2, 2020 ... Walmart doubled its sales in 1995 three years after the death of founder Sam Walton. Company management had taken a risk in doing so, electing ... The Walmart chain proper was founded in 1962 with a single store in Rogers, expanding inside Oklahoma by 1968 and throughout the rest of the Southern United ... Store purchases can be added to your purchase history to make it easier for you to manage your purchases online. Use the receipt lookup tool to locate a ... Walmart has increased its annual cash dividend every year since first declaring a $0.05 per share annual dividend in March 1974. For additional historic ... Your Walmart Digital Vaccination Record allows you to provide proof of vaccination in a secure, digital format. Access account. Sign in to your Walmart Pharmacy ... Stock Splits. Walmart Inc. (formerly Wal-Mart Stores, Inc.) was incorporated on Oct. 31, 1969. On Oct. 1, 1970, Walmart offered 300,000 shares of its common ... aka: Wal-Mart Stores, Inc. Founded in 1962 by Sam Walton, Walmart Inc., the world's most profitable retail outlet for many years, also became the largest ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use the Summarizer tool to summarize the information I found.\n",
      "Action: Summarizer\n",
      "Action Input: \"History of Walmart\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\n",
      "\n",
      "• Walmart was founded in 1962 by Sam Walton in Arkansas.\n",
      "• The first Walmart store was opened in 1962 and was called \"Walton's Five and Dime.\"\n",
      "• By 1970, Walmart had expanded to 38 stores and had sales of $44.2 million.\n",
      "• In 1972, Walmart became a publicly traded company and by 1980, it had reached $1 billion in annual sales.\n",
      "• Today, Walmart is the largest retailer in the world with over 11,000 stores in 27 countries and annual revenues of over $500 billion.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Walmart was founded in 1962 by Sam Walton in Arkansas and has since become the largest retailer in the world with over 11,000 stores in 27 countries and annual revenues of over $500 billion.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Walmart was founded in 1962 by Sam Walton in Arkansas and has since become the largest retailer in the world with over 11,000 stores in 27 countries and annual revenues of over $500 billion.\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"Write a summary of the following text in a bulleted list format in not more than five sentences: {query}\"\n",
    ")\n",
    "\n",
    "summarize_chain = LLMChain(llm=llm_openai, prompt=prompt)\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for finding information about recent events\"\n",
    "    ),\n",
    "    Tool(\n",
    "       name='Summarizer',\n",
    "       func=summarize_chain.run,\n",
    "       description='useful for summarizing texts'\n",
    "    )\n",
    "]\n",
    "\n",
    "agent = initialize_agent(tools, llm_openai, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "query= \"Write history of Walmart and summarize in a bulleted list format in not more than five sentences.\"\n",
    "response=  agent.run(query) \n",
    "print(response)\n",
    "# response = agent(\"What's the latest news about the Mars rover? Then please summarize the results.\")\n",
    "# print(response['output'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c2b14-2e43-4c64-8394-50234b01738a",
   "metadata": {},
   "source": [
    "### Pandas DataFrame Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66309ee6-1a4a-42e6-a39d-bd8678edcb0e",
   "metadata": {},
   "source": [
    "### SQL Database Agent\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
